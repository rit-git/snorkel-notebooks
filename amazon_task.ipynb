{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Customer Reviews Task\n",
    "\n",
    "### You will work with Amazon Customer Reviews, writing labeling functions that classify them as positive (1) or negative (0) sentiment. \n",
    "\n",
    "First, import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.preparer import load_amazon_dataset\n",
    "from snorkel.labeling import labeling_function\n",
    "from snorkel.labeling import LabelModel\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling import PandasLFApplier\n",
    "from analyzer import train_model\n",
    "import re\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "The reviews are available [via Amazon](https://s3.amazonaws.com/amazon-reviews-pds/readme.html).\n",
    "You may download them there, or provide a password to unzip the file below.\n",
    "\n",
    "For simplicity, only 1 star and 5 star reviews are included.\n",
    "\n",
    "You must replace `PASSWORD` with the password to unzip the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -P PASSWORD data/data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELIMITER = \"#\"\n",
    "df_train, df_dev, df_valid, df_test = load_amazon_dataset(delimiter=DELIMITER)\n",
    "print(\"{} training examples\".format(len(df_train)))\n",
    "print(\"{} development examples\".format(len(df_dev)))\n",
    "print(\"{} validation examples\".format(len(df_valid)))\n",
    "print(\"{} test examples\".format(len(df_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the labels for this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSTAIN = -1\n",
    "NEGATIVE = 0\n",
    "POSITIVE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Some labeled examples: \")\n",
    "display(df_dev[df_dev.label==NEGATIVE].sample(5))\n",
    "display(df_dev[df_dev.label==POSITIVE].sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Instructions\n",
    "\n",
    "All reviews were submitted with either 1 star (negative) or 5 star (positive) ratings. Your task is to create labeling functions that take the text of the review as input, and output either a NEGATIVE or a POSITIVE or an ABSTAIN label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Labeling Functions\n",
    "Time to write some labeling functions! Below is an example. Be sure to add your function to the list `lfs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def my_first_labeling_function(x):\n",
    "    return NEGATIVE if \"awful\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "lfs.append(my_first_labeling_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Functions\n",
    "This is how we obtain training labels, by training a model to combine the outputs of the noisy labeling functions.\n",
    "`L_train` and `L_dev` are matrices representing the label returned by each labeling function for each example in the training and development sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the LFs to the unlabeled training data, and the development data\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_train = applier.apply(df_train)\n",
    "L_dev = applier.apply(df_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the snorkel model to combine these noisy labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the label model and compute the training labels\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train, n_epochs=500, log_freq=50, seed=123)\n",
    "df_train[\"label\"] = label_model.predict(L=L_train, tie_break_policy=\"abstain\")\n",
    "display(df_train.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Unlabeled Examples\n",
    "You can use these to brainstorm new labeling functions. You may try filtering or sorting them in other ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can filter for unlabeled data\n",
    "df_unlabeled = df_train[df_train.label == ABSTAIN]\n",
    "display(df_unlabeled.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "Evaluate the accuracy of the estimated training labels and development set labels (based on ground truth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = df_train.label.values\n",
    "train_analysis = LFAnalysis(L=L_train, lfs=lfs).lf_summary(Y=Y_train)\n",
    "display(\"Training set results:\", train_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_dev = df_dev.label.values\n",
    "dev_analysis = LFAnalysis(L=L_dev, lfs=lfs).lf_summary(Y=Y_dev)\n",
    "display(\"Dev set results:\", dev_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "Lastly, train a simple bag of words model on these labels, and report test accuracy.\n",
    "\n",
    "(This step may take a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(label_model, df_train, df_valid, df_test, L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
