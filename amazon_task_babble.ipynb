{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Customer Reviews Classification with Babble\n",
    "\n",
    "### For this task, you will work with Amazon Customer Reviews, writing explanations about how to classify them as positive or negative sentiment.\n",
    "\n",
    "Only 1 star and 5 star reviews are included.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare the Data\n",
    "\n",
    "The reviews are available [via Amazon](https://s3.amazonaws.com/amazon-reviews-pds/readme.html).\n",
    "You may download them there, or provide a password to unzip the file below.\n",
    "\n",
    "For simplicity, only 1 star and 5 star reviews are included.\n",
    "\n",
    "You must replace `PASSWORD` with the password to unzip the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -P PASSWORD data/data.zip\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 training examples\n",
      "500 development examples\n",
      "399750 validation examples\n",
      "250 test examples\n"
     ]
    }
   ],
   "source": [
    "from data.preparer import load_amazon_dataset\n",
    "\n",
    "DELIMITER = \"#\"\n",
    "df_train, df_dev, df_valid, df_test = load_amazon_dataset(delimiter=DELIMITER)\n",
    "print(\"{} training examples\".format(len(df_train)))\n",
    "print(\"{} development examples\".format(len(df_dev)))\n",
    "print(\"{} validation examples\".format(len(df_valid)))\n",
    "print(\"{} test examples\".format(len(df_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define labels\n",
    "ABSTAIN = 0\n",
    "NEGATIVE = 1\n",
    "POSITIVE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the data into a format compatible with Babble Labble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from babble.Candidate import Candidate # this is a helper class to transform our data into a format Babble can parse\n",
    "\n",
    "dfs = [df_train, df_dev, df_test]\n",
    "\n",
    "for df in dfs:\n",
    "    df[\"id\"] = range(len(df))\n",
    "\n",
    "Cs = [df.apply(lambda x: Candidate(x), axis=1) for df in dfs]\n",
    "\n",
    "# babble labble uses 1 and 2 for labels, while our data uses 0 and 1\n",
    "# add 1 to convert\n",
    "Ys = [df.label.values + 1 for df in dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar construction complete.\n"
     ]
    }
   ],
   "source": [
    "from babble import BabbleStream\n",
    "\n",
    "aliases = {}\n",
    "babbler = BabbleStream(Cs, Ys, balanced=True, shuffled=True, seed=456, aliases=aliases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Instructions\n",
    "\n",
    "All reviews were submitted with either 1 star (negative) or 5 star (positive) ratings. Your task is to create labeling functions that take the text of the review as input, and output either a NEGATIVE or a POSITIVE or an ABSTAIN label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Explanations\n",
    "\n",
    "Creating explanations generally happens in five steps:\n",
    "1. View candidates\n",
    "2. Write explanations\n",
    "3. Get feedback\n",
    "4. Update explanations \n",
    "5. Apply label aggregator\n",
    "\n",
    "Steps 3-5 are optional; explanations may be submitted without any feedback on their quality. However, in our experience, observing how well explanations are being parsed and what their accuracy/coverage on a dev set are (if available) can quickly lead to simple improvements that yield significantly more useful labeling functions. Once a few labeling functions have been collected, you can use the label aggregator to identify candidates that are being mislabeled and write additional explanations targeting those failure modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection\n",
    "\n",
    "Use `babbler` to show candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 3223807, 'text': 'My treo was not at all durable. After one year the flip part did not work so the phone only worked with a headset. After a while the headset would not pick up calls. It also locked up and had to be rebooted in the middle of calls on a regular basis. There was a rebate with the phone and handspring never sent the rebate even with the appropriate documentation.', 'label': 0, 'id': 131, 'tokens': ['My', 'treo', 'was', 'not', 'at', 'all', 'durable', '.', 'After', 'one', 'year', 'the', 'flip', 'part', 'did', 'not', 'work', 'so', 'the', 'phone', 'only', 'worked', 'with', 'a', 'headset', '.', 'After', 'a', 'while', 'the', 'headset', 'would', 'not', 'pick', 'up', 'calls', '.', 'It', 'also', 'locked', 'up', 'and', 'had', 'to', 'be', 'rebooted', 'in', 'the', 'middle', 'of', 'calls', 'on', 'a', 'regular', 'basis', '.', 'There', 'was', 'a', 'rebate', 'with', 'the', 'phone', 'and', 'handspring', 'never', 'sent', 'the', 'rebate', 'even', 'with', 'the', 'appropriate', 'documentation', '.'], 'doc_id': 131, 'mention_id': 131}\n"
     ]
    }
   ],
   "source": [
    "candidate = babbler.next()\n",
    "print(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from babble import Explanation\n",
    "explanation = Explanation(\n",
    "    name='check_out', # name of this rule, for your reference\n",
    "    label=NEGATIVE, # label to assign\n",
    "    condition='The word \"bad\" is in the text', # natural language description of why you label the candidate this way\n",
    "    candidate=candidate.mention_id # optional argument, the candidate should be an example labeled by this rule\n",
    ")\n",
    "explanations.append(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Babble will parse your explanations into functions, then filter out functions that are duplicates, incorrectly label their given candidate, or assign the same label to all examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building list of target candidate ids...\n",
      "Collected 1 unique target candidate ids from 1 explanations.\n",
      "Gathering desired candidates...\n",
      "Found 1/1 desired candidates\n",
      "Linking explanations to candidates...\n",
      "Linked 1/1 explanations\n",
      "1 explanation(s) out of 1 were parseable.\n",
      "1 parse(s) generated from 1 explanation(s).\n",
      "1 parse(s) remain (0 parse(s) removed by DuplicateSemanticsFilter).\n",
      "Note: 1 LFs did not have candidates and therefore could not be filtered.\n",
      "1 parse(s) remain (0 parse(s) removed by ConsistencyFilter).\n",
      "Applying labeling functions to investigate labeling signature.\n",
      "[========================================] 100%\n",
      "\n",
      "1 parse(s) remain (0 parse(s) removed by UniformSignatureFilter: (0 None, 0 All)).\n",
      "1 parse(s) remain (0 parse(s) removed by DuplicateSignatureFilter).\n",
      "1 parse(s) remain (0 parse(s) removed by LowestCoverageFilter).\n"
     ]
    }
   ],
   "source": [
    "parses, filtered = babbler.apply(explanations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "See how your explanations were parsed and filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>check_out_0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>0.694444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             j  Polarity  Coverage  Overlaps  Conflicts  Correct  Incorrect  \\\n",
       "check_out_0  0       1.0     0.072       0.0        0.0       25         11   \n",
       "\n",
       "             Emp. Acc.  \n",
       "check_out_0   0.694444  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "babbler.analyze(parses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No filtered parses to analyze.\n"
     ]
    }
   ],
   "source": [
    "babbler.filtered_analysis(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1 parse(s) from 1 explanations to set. (Total # parses = 1)\n",
      "\n",
      "Applying labeling functions to split 1\n",
      "[========================================] 100%\n",
      "\n",
      "Added 36 labels to split 1: L.nnz = 36, L.shape = (500, 1).\n",
      "Applying labeling functions to split 2\n",
      "[========================================] 100%\n",
      "\n",
      "Added 16 labels to split 2: L.nnz = 16, L.shape = (250, 1).\n"
     ]
    }
   ],
   "source": [
    "babbler.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Get feedback on the performance of your explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved label matrix for split 0: L.nnz = 148, L.shape = (2500, 1)\n",
      "Retrieved label matrix for split 1: L.nnz = 36, L.shape = (500, 1)\n",
      "Retrieved label matrix for split 2: L.nnz = 16, L.shape = (250, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>check_out_0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>0.694444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             j  Polarity  Coverage  Overlaps  Conflicts  Correct  Incorrect  \\\n",
       "check_out_0  0         1     0.072       0.0        0.0       25         11   \n",
       "\n",
       "             Emp. Acc.  \n",
       "check_out_0   0.694444  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from metal.analysis import lf_summary\n",
    "\n",
    "Ls = [babbler.get_label_matrix(split) for split in [0,1,2]]\n",
    "lf_names = [lf.__name__ for lf in babbler.get_lfs()]\n",
    "lf_summary(Ls[1], Ys[1], lf_names=lf_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "[SUMMARY]\n",
      "Best model: [1]\n",
      "Best config: {'n_epochs': 500, 'show_plots': False, 'lr': 0.0012223249524949424, 'seed': 123}\n",
      "Best score: 0.6859395532194481\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from metal import LabelModel\n",
    "from metal.tuners import RandomSearchTuner\n",
    "\n",
    "search_space = {\n",
    "    'n_epochs': [50, 100, 500],\n",
    "    'lr': {'range': [0.01, 0.001], 'scale': 'log'},\n",
    "    'show_plots': False,\n",
    "}\n",
    "\n",
    "tuner = RandomSearchTuner(LabelModel, seed=123)\n",
    "\n",
    "label_aggregator = tuner.search(\n",
    "    search_space, \n",
    "    train_args=[Ls[0]], \n",
    "    X_dev=Ls[1], Y_dev=Ys[1], \n",
    "    max_search=20, verbose=False, metric='f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to save the explanations you've generated, you can use the `ExplanationIO` object to write to or read them from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from babble.utils import ExplanationIO\n",
    "\n",
    "FILE = \"my_explanations.tsv\"\n",
    "exp_io = ExplanationIO()\n",
    "exp_io.write(explanations, FILE)\n",
    "explanations = exp_io.read(FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
