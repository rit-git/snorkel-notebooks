{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_NAME = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from data.preparer import load_amazon_dataset\n",
    "\n",
    "from babble import Explanation\n",
    "from babble import BabbleStream\n",
    "from babble.Candidate import Candidate \n",
    "\n",
    "from analyzer import upload_data\n",
    "\n",
    "from metal.analysis import lf_summary\n",
    "from metal.analysis import label_coverage\n",
    "from metal import LabelModel\n",
    "from metal.tuners import RandomSearchTuner\n",
    "from babble.utils import ExplanationIO\n",
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "stat_history = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELIMITER = \"#\"\n",
    "df_train, df_dev, df_valid, df_test, _ = load_amazon_dataset(delimiter=DELIMITER)\n",
    "print(\"{} training examples\".format(len(df_train)))\n",
    "print(\"{} development examples\".format(len(df_dev)))\n",
    "print(\"{} validation examples\".format(len(df_valid)))\n",
    "print(\"{} test examples\".format(len(df_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the data into a format compatible with Babble Labble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_train, df_dev]\n",
    "dfs[0]['label'] = -1\n",
    "\n",
    "for df in dfs:\n",
    "    df[\"id\"] = range(len(df))\n",
    "    df[\"label\"] += 1\n",
    "\n",
    "Cs = [df.apply(lambda x: Candidate(x), axis=1) for df in dfs]\n",
    "\n",
    "# babble labble uses 1 and 2 for labels, while our data uses 0 and 1\n",
    "# add 1 to convert\n",
    "Ys = [df.label.values for df in dfs]\n",
    "Ys[0] -= 1 # no label (training set) should be set to -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Customer Reviews Classification with Babble\n",
    "\n",
    "### For this task, you will work with Amazon Customer Reviews, writing explanations about how to classify them as positive or negative sentiment.\n",
    "\n",
    "Only 1 star and 5 star reviews are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the timer!\n",
    "stat_history = stat_history.append({\n",
    "    \"time\": datetime.now(), \n",
    "    \"num_lfs\": 0,\n",
    "    \"f1\": 0.0,\n",
    "    \"precision\": 0.0,\n",
    "    \"recall\": 0.0,\n",
    "    \"training_label_coverage\": 0.0,\n",
    "    \"training_label_size\": 0.0\n",
    "}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define labels\n",
    "ABSTAIN = 0\n",
    "NEGATIVE = 1\n",
    "POSITIVE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that aliases are a way to refer to a set of words in a rule. \n",
    "\n",
    "For example, with\n",
    "`aliases = {\"couples\": [\"girlfriend\", \"boyfriend\", \"wife\", \"husband\"]}` \n",
    "\n",
    "--> now you can refer to \"couples\" in a rule, and the parser will know you mean any of these terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "babbler = BabbleStream(Cs, Ys, balanced=True, shuffled=True, seed=456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aliases = {} \n",
    "babbler.add_aliases(aliases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettyprint(candidate):\n",
    "    # just a helper function to print the candidate nicely\n",
    "    print(\"MENTION ID {}\".format(candidate.mention_id))\n",
    "    print()\n",
    "    print(candidate.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example candidate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate = babbler.next()\n",
    "prettyprint(candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Instructions\n",
    "\n",
    "All reviews were submitted with either 1 star (negative) or 5 star (positive) ratings. \n",
    "\n",
    "Your task is to __create labeling functions__ by writing natural language explanations of labeling rules. Try to write them as quickly and accurately as possible.\n",
    "\n",
    "You may consult the internet at any time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Explanations\n",
    "\n",
    "Creating explanations generally happens in five steps:\n",
    "1. View candidates\n",
    "2. Write explanations\n",
    "3. Get feedback\n",
    "4. Update explanations \n",
    "5. Apply label aggregator\n",
    "\n",
    "Steps 3-5 are optional; explanations may be submitted without any feedback on their quality. However, in our experience, observing how well explanations are being parsed and what their accuracy/coverage on a dev set are (if available) can quickly lead to simple improvements that yield significantly more useful labeling functions. Once a few labeling functions have been collected, you can use the label aggregator to identify candidates that are being mislabeled and write additional explanations targeting those failure modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection\n",
    "\n",
    "Use `babbler` to show candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "candidate = babbler.next()\n",
    "prettyprint(candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't know whether it's positive or negative, it's okay to make your best guess or skip an example.\n",
    "For a candidate you decide to label, write an explanation of why you chose that label.\n",
    "\n",
    "You can consult the internet or refer to the babble tutorial notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e0 = Explanation(\n",
    "    # feel free to change the name to something that describes your rule better.\n",
    "    name = \"e0\", \n",
    "    label = ABSTAIN, \n",
    "    condition = \"\", \n",
    "    # remember that is argument (candidate) is optional. \n",
    "    # You can use it to make sure the explanation applies to the candidate you pass as an argument.\n",
    "    #candidate = candidate.mention_id \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = Explanation(\n",
    "    name = \"e1\", \n",
    "    label = ABSTAIN, \n",
    "    condition = \"\", \n",
    "    #candidate = candidate.mention_id \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2 = Explanation(\n",
    "    name = \"e2\", \n",
    "    label = ABSTAIN, \n",
    "    condition = \"\", \n",
    "    #candidate = candidate.mention_id \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e3 = Explanation(\n",
    "    name = \"e3\", \n",
    "    label = ABSTAIN, \n",
    "    condition = \"\", \n",
    "    #candidate = candidate.mention_id \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e4 = Explanation(\n",
    "    name = \"e4\", \n",
    "    label = ABSTAIN, \n",
    "    condition = \"\", \n",
    "    #candidate = candidate.mention_id \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e5 = Explanation(\n",
    "    name = \"e5\", \n",
    "    label = ABSTAIN, \n",
    "    condition = \"\", \n",
    "    #candidate = candidate.mention_id \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e6 = Explanation(\n",
    "    name = \"e6\", \n",
    "    label = ABSTAIN, \n",
    "    condition = \"\", \n",
    "    #candidate = candidate.mention_id \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e7 = Explanation(\n",
    "    name = \"e7\", \n",
    "    label = ABSTAIN, \n",
    "    condition = \"\", \n",
    "    #candidate = candidate.mention_id \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e8 = Explanation(\n",
    "    name = \"e8\", \n",
    "    label = ABSTAIN, \n",
    "    condition = \"\", \n",
    "    #candidate = candidate.mention_id \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e9 = Explanation(\n",
    "    name = \"e9\", \n",
    "    label = ABSTAIN, \n",
    "    condition = \"\", \n",
    "    #candidate = candidate.mention_id \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e10 = Explanation(\n",
    "    name = \"e10\", \n",
    "    label = ABSTAIN, \n",
    "    condition = \"\", \n",
    "    #candidate = candidate.mention_id \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Babble will parse your explanations into functions, then filter out functions that are duplicates, incorrectly label their given candidate, or assign the same label to all examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any explanations that you haven't committed yet\n",
    "explanations = [e0, e1, e2, e3, e4, e5, e6, e7, e8, e9]\n",
    "\n",
    "parses, filtered = babbler.apply(explanations)\n",
    "stat_history = stat_history.append({\n",
    "    \"time\": datetime.now(), \n",
    "    \"num_lfs\": len(parses),\n",
    "    \"num_explanations\": len(explanations),\n",
    "    \"num_filtered\": len(filtered)\n",
    "}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "See how your explanations were parsed and filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    dev_analysis = babbler.analyze(parses)\n",
    "    display(dev_analysis)\n",
    "    dev_analysis['time'] = datetime.now()\n",
    "    dev_analysis['eval'] = \"dev\"\n",
    "    dev_analysis[\"lf_id\"] = dev_analysis.index\n",
    "    stat_history = stat_history.append(dev_analysis, sort=False, ignore_index=True)\n",
    "except ValueError as e:\n",
    "    print(\"It seems as though none of your labeling functions were parsed. See the cells above and below for more information.\")\n",
    "    print(\"ERROR:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "babbler.filtered_analysis(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "babbler.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Get feedback on the performance of your explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ls = [babbler.get_label_matrix(split) for split in [0,1,2]]\n",
    "lf_names = [lf.__name__ for lf in babbler.get_lfs()]\n",
    "lf_summary(Ls[1], Ys[1], lf_names=lf_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'n_epochs': [50, 100, 500],\n",
    "    'lr': {'range': [0.01, 0.001], 'scale': 'log'},\n",
    "    'show_plots': False,\n",
    "}\n",
    "\n",
    "tuner = RandomSearchTuner(LabelModel, seed=123)\n",
    "\n",
    "label_aggregator = tuner.search(\n",
    "    search_space, \n",
    "    train_args=[Ls[0]], \n",
    "    X_dev=Ls[1], Y_dev=Ys[1], \n",
    "    max_search=20, verbose=False, metric='f1')\n",
    "\n",
    "# record statistics over time\n",
    "pr, re, f1, acc = label_aggregator.score(Ls[1], Ys[1], metric=['precision', 'recall', 'f1', 'accuracy'])\n",
    "stats = {\n",
    "    \"precision\": pr,\n",
    "    \"recall\": re,\n",
    "    \"f1\": f1,\n",
    "    \"accuracy\": acc,\n",
    "    \"eval\": \"dev\",\n",
    "    \"model\": \"label_aggregator\",\n",
    "    \"time\": datetime.now(),\n",
    "    \"training_label_coverage\": label_coverage(Ls[0]),\n",
    "    \"training_label_size\": label_coverage(Ls[0])*len(dfs[0])\n",
    "}\n",
    "stat_history = stat_history.append(stats, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view some incorrectly labeled examples for a given LF\n",
    "j = 0\n",
    "print(lf_names[j])\n",
    "# set j to match the value of the LF you're interested in\n",
    "L_dev = Ls[1].todense()\n",
    "display(df_dev[L_dev[:,j].A1==abs(df_dev[\"label\"]-3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "We can train a simple bag of words model on these labels, and see test accuracy.\n",
    "\n",
    "(This step may take a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_train = Ls[0].todense()\n",
    "probs_train = label_aggregator.predict_proba(L=L_train)\n",
    "mask = (L_train != 0).any(axis=1).A1\n",
    "df_train_filtered = df_train.iloc[mask]\n",
    "probs_train_filtered = probs_train[mask]\n",
    "print(\"{} out of {} examples used for training data\".format(len(df_train_filtered), len(df_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analyzer import train_model_from_probs\n",
    "stats = train_model_from_probs(df_train_filtered, probs_train_filtered, df_valid, df_test)\n",
    "stats[\"time\"] = datetime.now()\n",
    "stat_history = stat_history.append(stats, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save\n",
    "When your time is up, please save your explanations and model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir babble_amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_history.to_csv(\"babble_amazon/statistics_history.csv\")\n",
    "%history -p -o -f babble_amazon/history.log\n",
    "!cp babble_amazon_task.ipynb babble_amazon/notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save explanations\n",
    "FILE = \"babble_amazon/explanations.tsv\"\n",
    "from types import SimpleNamespace\n",
    "exp_io = ExplanationIO()\n",
    "for exp in explanations:\n",
    "    if exp.candidate is None:\n",
    "        exp.candidate = SimpleNamespace(mention_id = None)\n",
    "exp_io.write(explanations, FILE)\n",
    "explanations = exp_io.read(FILE)\n",
    "\n",
    "# save label model\n",
    "label_aggregator.save(\"babble_amazon/lfmodel.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r babble_amazon.zip babble_amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(YOUR_NAME) > 0\n",
    "upload_data(\"babble_amazon.zip\", YOUR_NAME + \"_babble_amazon.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
