{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Babble Tutorial\n",
    "\n",
    "### You will work with Wikipedia plot descriptions of films that are either comedy or drama.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.preparer import load_film_dataset\n",
    "from babble import Explanation\n",
    "from babble import BabbleStream\n",
    "from babble.Candidate import Candidate \n",
    "\n",
    "from metal.analysis import lf_summary\n",
    "from metal.analysis import label_coverage\n",
    "from metal import LabelModel\n",
    "from metal.tuners import RandomSearchTuner\n",
    "from babble.utils import ExplanationIO\n",
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "stat_history = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "These movie plot descriptions are from [Kaggle](https://www.kaggle.com/jrobischon/wikipedia-movie-plots).\n",
    "You will be labeling films as either \"comedy\" or \"drama\" based on their plot descriptions.\n",
    "\n",
    "If you're not sure about the correct label, that's fine -- either make your best guess or just skip the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# Unzip the data. (Don't worry about this, it should be already unzipped.)\n",
    "# Replace PASSWORD with the password to unzip the data, or download it directly from Kaggle.\n",
    "\n",
    "#!unzip -P PASSWORD data/data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset into training, validation, development, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_dev, df_valid, df_test = load_film_dataset()\n",
    "print(\"{} training examples\".format(len(df_train)))\n",
    "print(\"{} development examples\".format(len(df_dev)))\n",
    "print(\"{} validation examples\".format(len(df_valid)))\n",
    "print(\"{} test examples\".format(len(df_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the labels for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSTAIN = 0\n",
    "DRAMA = 1\n",
    "COMEDY = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the data into a format compatible with Babble Labble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a helper class to transform our data into a format Babble can parse\n",
    "\n",
    "dfs = [df_train, df_dev, df_test]\n",
    "\n",
    "for df in dfs:\n",
    "    df[\"id\"] = range(len(df))\n",
    "\n",
    "Cs = [df.apply(lambda x: Candidate(x), axis=1) for df in dfs]\n",
    "\n",
    "# babble labble uses 1 and 2 for labels, while our data uses 0 and 1\n",
    "# add 1 to convert\n",
    "Ys = [df.label.values + 1 for df in dfs]\n",
    "Ys[0] -= 1 # no label (training set) should be set to -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data into a *BabbleStream*: an object that iteratively displays candidates, collects and parses explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aliases are a way to refer to a set of words in a rule.\n",
    "aliases = {\"couples\": [\"girlfriend\", \"boyfriend\", \"wife\", \"husband\"]}\n",
    "babbler = BabbleStream(Cs, Ys, balanced=True, shuffled=True, seed=456, aliases=aliases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettyprint(candidate):\n",
    "    # just a helper function to print the candidate nicely\n",
    "    print(candidate.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example candidate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate = babbler.next()\n",
    "prettyprint(candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll learn how to write a labelling function from a natural language explanation of why you chose a label for a given candidate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Explanations\n",
    "\n",
    "Creating explanations generally happens in five steps:\n",
    "1. View candidates\n",
    "2. Write explanations\n",
    "3. Get feedback\n",
    "4. Update explanations \n",
    "5. Apply label aggregator\n",
    "\n",
    "Steps 3-5 are optional; explanations may be submitted without any feedback on their quality. However, in our experience, observing how well explanations are being parsed and what their accuracy/coverage on a dev set are (if available) can quickly lead to simple improvements that yield significantly more useful labeling functions. Once a few labeling functions have been collected, you can use the label aggregator to identify candidates that are being mislabeled and write additional explanations targeting those failure modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your task for this tutorial is to write 5 labeling functions.__\n",
    "\n",
    "Feel free to consult the internet or ask your experiment leader.\n",
    "\n",
    "*(For the real task, you will be asked to write 10 labeling functions, as quickly and accurately as possible. You will still be allowed to use the internet in this phase, but not ask your experiment leader.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection\n",
    "\n",
    "Use `babbler` to show candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "candidate = babbler.next()\n",
    "prettyprint(candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it a comedy or a drama? What makes you think that? (If you don't know, it's okay to make your best guess or skip an example.)\n",
    "\n",
    "Run the three examples given below, then parse them, and analyze them.\n",
    "Then, you can try editing them and writing your own functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e0 = Explanation(\n",
    "    # name of this rule, for your reference\n",
    "    name='bounty_hunter', \n",
    "    # label to assign\n",
    "    label=DRAMA, \n",
    "    # natural language description of why you label the candidate this way\n",
    "    condition='The phrase \"bounty hunter\" is in the text', \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = Explanation(\n",
    "    name = 'feelings', \n",
    "    label = DRAMA, \n",
    "    condition = 'because \"have\" or \"had\" or \"has\" occurs between \"they\" and \"feelings\"', \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of an explanation that uses an alias: \"couples\"\n",
    "\n",
    "You can define more aliases where the BabbleStream is initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2 = Explanation(\n",
    "    name = 'couple', \n",
    "    label = COMEDY, \n",
    "    condition = 'couples occur in the text', \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e3 = Explanation(\n",
    "    name = \"e3\", \n",
    "    label = ABSTAIN, \n",
    "    condition = \"\", \n",
    "    # candidate is an optional argument, it should be the id of an example labeled by this rule.\n",
    "    # if the rule doesn't apply to the candidate you provide, it will be filtered!\n",
    "    candidate = candidate.mention_id \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e4 = Explanation(\n",
    "    name = \"e4\", \n",
    "    label = ABSTAIN, \n",
    "    condition = \"\", \n",
    "    candidate = candidate.mention_id \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations = [e0, e1, e2, e3, e4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Babble will parse your explanations into functions, then filter out functions that are duplicates, incorrectly label their given candidate, or assign the same label to all examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parses, filtered = babbler.apply(explanations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "See how your parsed explanations performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "babbler.analyze(parses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See which explanations were filtered and why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "babbler.filtered_analysis(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "babbler.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Get feedback on the performance of your explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ls = [babbler.get_label_matrix(split) for split in [0,1,2]]\n",
    "lf_names = [lf.__name__ for lf in babbler.get_lfs()]\n",
    "lf_summary(Ls[1], Ys[1], lf_names=lf_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'n_epochs': [50, 100, 500],\n",
    "    'lr': {'range': [0.01, 0.001], 'scale': 'log'},\n",
    "    'show_plots': False,\n",
    "}\n",
    "\n",
    "tuner = RandomSearchTuner(LabelModel, seed=123)\n",
    "\n",
    "label_aggregator = tuner.search(\n",
    "    search_space, \n",
    "    train_args=[Ls[0]], \n",
    "    X_dev=Ls[1], Y_dev=Ys[1], \n",
    "    max_search=20, verbose=False, metric='f1')\n",
    "\n",
    "# record statistics over time\n",
    "pr, re, f1 = label_aggregator.score(Ls[1], Ys[1], metric=['precision', 'recall', 'f1'])\n",
    "stats = {\n",
    "    \"precision\": pr,\n",
    "    \"recall\": re,\n",
    "    \"f1\": f1,\n",
    "    \"time\": datetime.now(),\n",
    "    \"training_label_coverage\": label_coverage(Ls[0]),\n",
    "    \"training_label_size\": label_coverage(Ls[0])*len(dfs[0])\n",
    "}\n",
    "stat_history = stat_history.append(stats, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to save the explanations you've generated, you can use the `ExplanationIO` object to write to or read them from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_history.to_csv(\"babbler_tutorial_statistics_history.csv\")\n",
    "FILE = \"babbler_tutorial_explanations.tsv\"\n",
    "exp_io = ExplanationIO()\n",
    "exp_io.write(explanations, FILE)\n",
    "explanations = exp_io.read(FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
