{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from data.preparer import load_news_dataset\n",
    "from babble import Explanation\n",
    "from babble import BabbleStream\n",
    "from babble.Candidate import Candidate \n",
    "\n",
    "from metal.analysis import lf_summary\n",
    "from metal.analysis import label_coverage\n",
    "from metal import LabelModel\n",
    "from metal.tuners import RandomSearchTuner\n",
    "from babble.utils import ExplanationIO\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "\n",
    "stat_history = pd.DataFrame()\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "These texts discuss either gun politics (1) or computer electronics (0).\n",
    "\n",
    "If you're not sure about the correct label, that's fine -- either make your best guess or just skip the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# Unzip the data. (Don't worry about this, it should be already unzipped.)\n",
    "# Replace PASSWORD with the password to unzip the data, or download it directly from Kaggle.\n",
    "\n",
    "#!unzip -P PASSWORD data/data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset into training, validation, development, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_dev, df_valid, df_test, _ = load_news_dataset()\n",
    "print(\"{} training examples\".format(len(df_train)))\n",
    "print(\"{} development examples\".format(len(df_dev)))\n",
    "print(\"{} validation examples\".format(len(df_valid)))\n",
    "print(\"{} test examples\".format(len(df_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the data and labels into a Babble-friendly format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_train, df_dev]\n",
    "dfs[0]['label'] = -1\n",
    "\n",
    "for df in dfs:\n",
    "    df[\"id\"] = range(len(df))\n",
    "    df[\"label\"] += 1\n",
    "\n",
    "Cs = [df.apply(lambda x: Candidate(x), axis=1) for df in dfs]\n",
    "\n",
    "# babble labble uses 1 and 2 for labels, while our data uses 0 and 1\n",
    "# add 1 to convert\n",
    "Ys = [df.label.values for df in dfs]\n",
    "Ys[0] -= 1 # no label (training set) should be set to -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the labels for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSTAIN = 0\n",
    "ELECTRONICS = 1\n",
    "GUNS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Babble Tutorial\n",
    "## News forum classification\n",
    "\n",
    "### You will work with a subset of the 20 NewsGroup dataset. \n",
    "The texts shown are from one of two forums:\n",
    " 1. Computer Electronics (Label 1)\n",
    " 2. Gun Politics Forum (Label 2)\n",
    "Your job is to create a training data set to classify texts as belonging to one of these two forums.\n",
    "\n",
    "__You will do this by writing natural language explanations of why you would label an example a certain way (1 (ELECTRONICS), 2 (GUNS), or 0 (ABSTAIN or no label)).__\n",
    "These explanations will be parsed into functions which will be aggregated by Snorkel to create training data from unlabeled examples.\n",
    "\n",
    "You can evaluate your progress based on the coverage and f1 score of your label model, or by training a logistic regression classifier on the data and evaluating the test result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the timer!\n",
    "stat_history = stat_history.append({\n",
    "    \"time\": datetime.now(), \n",
    "    \"num_lfs\": 0,\n",
    "    \"f1\": 0.0,\n",
    "    \"precision\": 0.0,\n",
    "    \"recall\": 0.0,\n",
    "    \"training_label_coverage\": 0.0,\n",
    "    \"training_label_size\": 0.0\n",
    "}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data into a *BabbleStream*: an object that iteratively displays candidates, collects and parses explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "babbler = BabbleStream(Cs, Ys, balanced=True, shuffled=True, seed=456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can define aliases (a concise way to refer to a set of terms). \n",
    "In a little bit you'll see an example of how to use aliases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aliases are a way to refer to a set of words in a rule.\n",
    "aliases = {\n",
    "    \"unit\": [\"joules\", \"volts\", \"ohms\", \"MHz\"]\n",
    "}\n",
    "babbler.add_aliases(aliases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettyprint(candidate):\n",
    "    # just a helper function to print the candidate nicely\n",
    "    print(\"MENTION ID {}\".format(candidate.mention_id))\n",
    "    print()\n",
    "    print(candidate.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example candidate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun this cell to get a new example\n",
    "candidate = babbler.next()\n",
    "prettyprint(candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Next, we'll learn how to write a labelling function from a natural language explanation of why you chose a label for a given candidate.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Explanations\n",
    "\n",
    "Creating explanations generally happens in five steps:\n",
    "1. View candidates\n",
    "2. Write explanations\n",
    "3. Get feedback\n",
    "4. Update explanations \n",
    "5. Apply label aggregator\n",
    "\n",
    "Steps 3-5 are optional; explanations may be submitted without any feedback on their quality. However, in our experience, observing how well explanations are being parsed and what their accuracy/coverage on a dev set are (if available) can quickly lead to simple improvements that yield significantly more useful labeling functions. \n",
    "\n",
    "Once a few labeling functions have been collected, you can use the label aggregator to identify candidates that are being mislabeled and write additional explanations targeting those failure modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to consult the internet or ask your experiment leader.\n",
    "\n",
    "*For the real task, you will be asked to write labeling functions as quickly and accurately as possible. You will still be allowed to use the internet in this phase, but not ask your experiment leader. You may refer to this tutorial as needed.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection\n",
    "\n",
    "Use `babbler` to show candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "candidate = babbler.next()\n",
    "prettyprint(candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it about guns or electronics? What makes you think that? (If you don't know, it's okay to make your best guess or skip an example.)\n",
    "\n",
    "Run the three examples given below, then parse them, and analyze them.\n",
    "Then, you can try editing them and writing your own functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e0 = Explanation(\n",
    "    # name of this rule, for your reference\n",
    "    name='electr...', \n",
    "    \n",
    "    # label to assign\n",
    "    label=ELECTRONICS, \n",
    "    \n",
    "    # natural language description of why you label the candidate this way\n",
    "    condition='A word in the sentence starts with \"electr\"', \n",
    "    \n",
    "    # candidate is an optional argument, it should be the id of an example labeled by this rule.\n",
    "    # This is a fail-safe: if the rule doesn't apply to the candidate you provide, it will be filtered!\n",
    "    candidate = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = Explanation(\n",
    "    name = 'politics', \n",
    "    label = GUNS, \n",
    "    condition = 'Any of the words \"election\", \"senator\", \"democrat\", \"candidate\", or \"republican\" are in the text', \n",
    "    candidate = 33 # the candidate's mention ID, optional argument\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2 = Explanation(\n",
    "    name = 'selfdefense', \n",
    "    label = GUNS, \n",
    "    condition = 'because the word \"self\" occurs before \"defense\"'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of an explanation that uses an alias: \"unit\".\n",
    "\n",
    "You can define more aliases where the BabbleStream is initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e3 = Explanation(\n",
    "    name = \"units\", \n",
    "    label = ELECTRONICS, \n",
    "    condition = 'A word in the sentence is a unit' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e4 = Explanation(\n",
    "    name = \"e4\", \n",
    "    label = ABSTAIN, \n",
    "    condition = \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Babble will parse your explanations into functions, then filter out functions that are duplicates, incorrectly label their given candidate, or assign the same label to all examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any explanations that you haven't committed yet\n",
    "explanations = [e0, e1, e2, e3]\n",
    "\n",
    "parses, filtered = babbler.apply(explanations)\n",
    "stat_history = stat_history.append({\n",
    "    \"time\": datetime.now(), \n",
    "    \"num_lfs\": len(parses),\n",
    "    \"num_explanations\": len(explanations),\n",
    "    \"num_filtered\": len(filtered)\n",
    "}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "See how your parsed explanations performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    dev_analysis = babbler.analyze(parses)\n",
    "    display(dev_analysis)\n",
    "    dev_analysis['time'] = datetime.now()\n",
    "    dev_analysis['eval'] = \"dev\"\n",
    "    dev_analysis[\"lf_id\"] = dev_analysis.index\n",
    "    stat_history = stat_history.append(dev_analysis, sort=False, ignore_index=True)\n",
    "except ValueError as e:\n",
    "    print(\"It seems as though none of your labeling functions were parsed. See the cells above and below for more information.\")\n",
    "    print(\"ERROR:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See which explanations were filtered and why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "babbler.filtered_analysis(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "babbler.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Get feedback on the performance of your explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'n_epochs': [50, 100, 500],\n",
    "    'lr': {'range': [0.01, 0.001], 'scale': 'log'},\n",
    "    'show_plots': False,\n",
    "}\n",
    "\n",
    "tuner = RandomSearchTuner(LabelModel, seed=123)\n",
    "\n",
    "label_aggregator = tuner.search(\n",
    "    search_space, \n",
    "    train_args=[Ls[0]], \n",
    "    X_dev=Ls[1], Y_dev=Ys[1], \n",
    "    max_search=20, verbose=False, metric='f1')\n",
    "\n",
    "# record statistics over time\n",
    "pr, re, f1, acc = label_aggregator.score(Ls[1], Ys[1], metric=['precision', 'recall', 'f1', 'accuracy'])\n",
    "stats = {\n",
    "    \"precision\": pr,\n",
    "    \"recall\": re,\n",
    "    \"f1\": f1,\n",
    "    \"accuracy\": acc,\n",
    "    \"eval\": \"dev\",\n",
    "    \"model\": \"label_aggregator\",\n",
    "    \"time\": datetime.now(),\n",
    "    \"training_label_coverage\": label_coverage(Ls[0]),\n",
    "    \"training_label_size\": label_coverage(Ls[0])*len(dfs[0])\n",
    "}\n",
    "stat_history = stat_history.append(stats, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view some incorrectly labeled examples for a given LF\n",
    "j = 0\n",
    "print(lf_names[j])\n",
    "# set j to match the value of the LF you're interested in\n",
    "L_dev = Ls[1].todense()\n",
    "display(df_dev[L_dev[:,j].A1==abs(df_dev[\"label\"]-3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "We can train a simple bag of words model on these labels, and see test accuracy.\n",
    "\n",
    "(This step may take a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_train = Ls[0].todense()\n",
    "probs_train = label_aggregator.predict_proba(L=L_train)\n",
    "mask = (L_train != 0).any(axis=1).A1\n",
    "df_train_filtered = df_train.iloc[mask]\n",
    "probs_train_filtered = probs_train[mask]\n",
    "print(\"{} out of {} examples used for training data\".format(len(df_train_filtered), len(df_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analyzer import train_model_from_probs\n",
    "stats = train_model_from_probs(df_train_filtered, probs_train_filtered, df_valid, df_test)\n",
    "stats[\"time\"] = datetime.now()\n",
    "stat_history = stat_history.append(stats, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save\n",
    "When your time is up, please save your explanations and model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save statistics history \n",
    "stat_history.to_csv(\"babble_tutorial_statistics_history.csv\")\n",
    "\n",
    "# save explanations\n",
    "FILE = \"babble_tutorial_explanations.tsv\"\n",
    "from types import SimpleNamespace\n",
    "exp_io = ExplanationIO()\n",
    "for exp in explanations:\n",
    "    if exp.candidate is None:\n",
    "        exp.candidate = SimpleNamespace(mention_id = None)\n",
    "exp_io.write(explanations, FILE)\n",
    "explanations = exp_io.read(FILE)\n",
    "\n",
    "# save label model\n",
    "label_aggregator.save(\"babble_tutorial_lfmodel.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
