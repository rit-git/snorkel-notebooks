{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sara/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from data.preparer import load_news_dataset\n",
    "from babble import Explanation\n",
    "from babble import BabbleStream\n",
    "from babble.Candidate import Candidate \n",
    "\n",
    "from metal.analysis import lf_summary\n",
    "from metal.analysis import label_coverage\n",
    "from metal import LabelModel\n",
    "from metal.tuners import RandomSearchTuner\n",
    "from babble.utils import ExplanationIO\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "\n",
    "stat_history = pd.DataFrame()\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "stat_history = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "These texts discuss either gun politics (1) or computer electronics (0).\n",
    "\n",
    "If you're not sure about the correct label, that's fine -- either make your best guess or just skip the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# Unzip the data. (Don't worry about this, it should be already unzipped.)\n",
    "# Replace PASSWORD with the password to unzip the data, or download it directly from Kaggle.\n",
    "\n",
    "#!unzip -P PASSWORD data/data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset into training, validation, development, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "477 training examples\n",
      "400 development examples\n",
      "100 validation examples\n",
      "100 test examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sara/Documents/GitHub/snorkel-notebooks/venv/lib/python3.6/site-packages/pandas/core/frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "df_train, df_dev, df_valid, df_test = load_news_dataset()\n",
    "print(\"{} training examples\".format(len(df_train)))\n",
    "print(\"{} development examples\".format(len(df_dev)))\n",
    "print(\"{} validation examples\".format(len(df_valid)))\n",
    "print(\"{} test examples\".format(len(df_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a helper class to transform our data into a format Babble can parse\n",
    "df_train[\"label\"] = -2\n",
    "dfs = [df_train, df_dev, df_test]\n",
    "\n",
    "for df in dfs:\n",
    "    df[\"id\"] = range(len(df))\n",
    "\n",
    "Cs = [df.apply(lambda x: Candidate(x), axis=1) for df in dfs]\n",
    "\n",
    "# babble labble uses 1 and 2 for labels, while our data uses 0 and 1\n",
    "# add 1 to convert\n",
    "Ys = [df.label.values + 1 for df in dfs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the labels for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSTAIN = 0\n",
    "ELECTRONICS = 1\n",
    "GUNS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Babble Tutorial\n",
    "## News forum classification\n",
    "\n",
    "### You will work with a subset of the 20 NewsGroup dataset. \n",
    "The texts shown are from one of two forums:\n",
    " 1. Computer Electronics (Label 1)\n",
    " 2. Gun Politics Forum (Label 2)\n",
    "Your job is to create a training data set to classify texts as belonging to one of these two forums.\n",
    "\n",
    "You will do this by writing natural language explanations of why you would label an example a certain way (1 (ELECTRONICS), 2 (GUNS), or 0 (ABSTAIN or no label)).\n",
    "These explanations will be parsed into functions which will be aggregated by Snorkel to create training data from unlabeled examples.\n",
    "\n",
    "You can evaluate your progress based on the coverage and f1 score of your label model, or by training a logistic regression classifier on the data and evaluating the test result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_lfs</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-02-01 16:32:14.032294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_lfs                       time\n",
       "0      0.0 2020-02-01 16:32:14.032294"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start timer!\n",
    "stat_history.append({\"time\": datetime.now(), \"num_lfs\": 0}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data into a *BabbleStream*: an object that iteratively displays candidates, collects and parses explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar construction complete.\n"
     ]
    }
   ],
   "source": [
    "# aliases are a way to refer to a set of words in a rule.\n",
    "aliases = {\n",
    "    \"units\": [\"joules\", \"volts\", \"ohms\", \"MHz\"]\n",
    "}\n",
    "\n",
    "\n",
    "babbler = BabbleStream(Cs, Ys, balanced=True, shuffled=True, seed=456, aliases=aliases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettyprint(candidate):\n",
    "    # just a helper function to print the candidate nicely\n",
    "    print(candidate.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example candidate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a followup post to something I've written previously.  Several\n",
      "people responded with good information, but I don't think I communicated \n",
      "exactly what I am looking for.\n",
      "\tI'm working on a custom I/O device that will communicate with a host\n",
      "via RS-232.  My custom circuitry will use an 80C186EB or EC CPU and require\n",
      "about 64K of RAM (preferably FLASH RAM).  In looking around, I see that lots\n",
      "of people have engineered UART-CPU-ROM-RAM subsystems that are ready to be\n",
      "interfaced to your custom I/O devices.  It's been done so much, that it \n",
      "would be best if I can avoid reinventing a system.  It just needs to use\n",
      "an 80C186 (or 188) CPU, and be able to load a program from the host then \n",
      "transfer control to that program.  Well, there's one other thing the ROM\n",
      "needs to know how to do.  It should have routines to send and receive bytes\n",
      "to/from the host, that utilize the hardware control lines (DTR,RTS,DTS,CTS).\n",
      "Everything I've seen is in the $200.00 and up range.  That's too much for\n",
      "this application.  I need something around $100.00.  The CPU has the UART\n",
      "built-in, so you're only looking at a few chips.  Does anyone know a \n",
      "company that markets a good board in this range, or some public domain \n",
      "circuitry I can use?  Thanks in advance for the info.\n"
     ]
    }
   ],
   "source": [
    "# Rerun this cell to get a new example\n",
    "candidate = babbler.next()\n",
    "prettyprint(candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll learn how to write a labelling function from a natural language explanation of why you chose a label for a given candidate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Explanations\n",
    "\n",
    "Creating explanations generally happens in five steps:\n",
    "1. View candidates\n",
    "2. Write explanations\n",
    "3. Get feedback\n",
    "4. Update explanations \n",
    "5. Apply label aggregator\n",
    "\n",
    "Steps 3-5 are optional; explanations may be submitted without any feedback on their quality. However, in our experience, observing how well explanations are being parsed and what their accuracy/coverage on a dev set are (if available) can quickly lead to simple improvements that yield significantly more useful labeling functions. Once a few labeling functions have been collected, you can use the label aggregator to identify candidates that are being mislabeled and write additional explanations targeting those failure modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to consult the internet or ask your experiment leader.\n",
    "\n",
    "*(For the real task, you will be asked to write labeling functions, as quickly and accurately as possible. You will still be allowed to use the internet in this phase, but not ask your experiment leader.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection\n",
    "\n",
    "Use `babbler` to show candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I have the April 15, 1993 issue of the SF Chronicle in my lap.  Page\n",
      "E7 (in the \"Sporting Green\" section) has a Trader's advert.  (The\n",
      "copy is a bit screwed up - it says that the prices offered expire\n",
      "4-14-93, but the ad is there.)\n",
      "\n",
      "The SF Examiner and Chronicle run the same set of adverts (because\n",
      "they have a joint printing/biz agreement and differ only in editorial\n",
      "content).\n",
      "\n",
      "I've seen gun ads recently in the merc, which is anti-gun editorially,\n",
      "albeit not from traders, but from its competitors.\n",
      "\n",
      "I don't know about the other papers.\n",
      "\n",
      "Does Traders claim that things are changing?  When?\n",
      "\n",
      "\n",
      "Because it's easier than telling the truth and no one much cares\n",
      "either way.\n",
      "\n",
      "\n",
      "Before you do, make sure that the bozos are actually doing what\n",
      "you're accusing them of.\n",
      "\n",
      "-andy\n",
      "216\n"
     ]
    }
   ],
   "source": [
    "candidate = babbler.next()\n",
    "prettyprint(candidate)\n",
    "print(candidate.mention_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it about guns or electronics? What makes you think that? (If you don't know, it's okay to make your best guess or skip an example.)\n",
    "\n",
    "Run the three examples given below, then parse them, and analyze them.\n",
    "Then, you can try editing them and writing your own functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "e0 = Explanation(\n",
    "    # name of this rule, for your reference\n",
    "    name='electr...', \n",
    "    # label to assign\n",
    "    label=ELECTRONICS, \n",
    "    # natural language description of why you label the candidate this way\n",
    "    condition='A word in the sentence starts with \"electr\"', \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = Explanation(\n",
    "    name = 'politics', \n",
    "    label = GUNS, \n",
    "    condition = 'Any of the words \"election\", \"senator\", \"democrat\", or \"republican\" are in the text', \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of an explanation that uses an alias: \"couples\"\n",
    "\n",
    "You can define more aliases where the BabbleStream is initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2 = Explanation(\n",
    "    name = 'selfdefense', \n",
    "    label = GUNS, \n",
    "    condition = 'because the word \"self\" occurs before \"defense\"', \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "e3 = Explanation(\n",
    "    name = \"e3\", \n",
    "    label = ABSTAIN, \n",
    "    condition = '', \n",
    "    # candidate is an optional argument, it should be the id of an example labeled by this rule.\n",
    "    # if the rule doesn't apply to the candidate you provide, it will be filtered!\n",
    "    candidate = candidate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "e4 = Explanation(\n",
    "    name = \"e4\", \n",
    "    label = ABSTAIN, \n",
    "    condition = \"\", \n",
    "    candidate = candidate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Babble will parse your explanations into functions, then filter out functions that are duplicates, incorrectly label their given candidate, or assign the same label to all examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building list of target candidate ids...\n",
      "Collected 1 unique target candidate ids from 4 explanations.\n",
      "Gathering desired candidates...\n",
      "Could not find 1 target candidates with the following mention_ids (first 5):\n",
      "{'text': '\\nI have the April 15, 1993 issue of the SF Chronicle in my lap.  Page\\nE7 (in the \"Sporting Green\" section) has a Trader\\'s advert.  (The\\ncopy is a bit screwed up - it says that the prices offered expire\\n4-14-93, but the ad is there.)\\n\\nThe SF Examiner and Chronicle run the same set of adverts (because\\nthey have a joint printing/biz agreement and differ only in editorial\\ncontent).\\n\\nI\\'ve seen gun ads recently in the merc, which is anti-gun editorially,\\nalbeit not from traders, but from its competitors.\\n\\nI don\\'t know about the other papers.\\n\\nDoes Traders claim that things are changing?  When?\\n\\n\\nBecause it\\'s easier than telling the truth and no one much cares\\neither way.\\n\\n\\nBefore you do, make sure that the bozos are actually doing what\\nyou\\'re accusing them of.\\n\\n-andy', 'label': 1, 'id': 216, 'tokens': ['I', 'have', 'the', 'April', '15', ',', '1993', 'issue', 'of', 'the', 'SF', 'Chronicle', 'in', 'my', 'lap', '.', 'Page', 'E7', '(', 'in', 'the', '``', 'Sporting', 'Green', \"''\", 'section', ')', 'has', 'a', 'Trader', \"'s\", 'advert', '.', '(', 'The', 'copy', 'is', 'a', 'bit', 'screwed', 'up', '-', 'it', 'says', 'that', 'the', 'prices', 'offered', 'expire', '4-14-93', ',', 'but', 'the', 'ad', 'is', 'there', '.', ')', 'The', 'SF', 'Examiner', 'and', 'Chronicle', 'run', 'the', 'same', 'set', 'of', 'adverts', '(', 'because', 'they', 'have', 'a', 'joint', 'printing/biz', 'agreement', 'and', 'differ', 'only', 'in', 'editorial', 'content', ')', '.', 'I', \"'ve\", 'seen', 'gun', 'ads', 'recently', 'in', 'the', 'merc', ',', 'which', 'is', 'anti-gun', 'editorially', ',', 'albeit', 'not', 'from', 'traders', ',', 'but', 'from', 'its', 'competitors', '.', 'I', 'do', \"n't\", 'know', 'about', 'the', 'other', 'papers', '.', 'Does', 'Traders', 'claim', 'that', 'things', 'are', 'changing', '?', 'When', '?', 'Because', 'it', \"'s\", 'easier', 'than', 'telling', 'the', 'truth', 'and', 'no', 'one', 'much', 'cares', 'either', 'way', '.', 'Before', 'you', 'do', ',', 'make', 'sure', 'that', 'the', 'bozos', 'are', 'actually', 'doing', 'what', 'you', \"'re\", 'accusing', 'them', 'of', '.', '-andy'], 'doc_id': 216, 'mention_id': 216}\n",
      "Found 0/1 desired candidates\n",
      "Linking explanations to candidates...\n",
      "Linked 0/4 explanations\n",
      "3 explanation(s) out of 4 were parseable.\n",
      "5 parse(s) generated from 4 explanation(s).\n",
      "4 parse(s) remain (1 parse(s) removed by DuplicateSemanticsFilter).\n",
      "Note: 4 LFs did not have candidates and therefore could not be filtered.\n",
      "4 parse(s) remain (0 parse(s) removed by ConsistencyFilter).\n",
      "Applying labeling functions to investigate labeling signature.\n",
      "[========================================] 100%\n",
      "\n",
      "3 parse(s) remain (1 parse(s) removed by UniformSignatureFilter: (1 None, 0 All)).\n",
      "2 parse(s) remain (1 parse(s) removed by DuplicateSignatureFilter).\n",
      "2 parse(s) remain (0 parse(s) removed by LowestCoverageFilter).\n"
     ]
    }
   ],
   "source": [
    "# Add any explanations that you haven't committed yet\n",
    "explanations = [e0, e1, e2, e3]\n",
    "\n",
    "parses, filtered = babbler.apply(explanations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "See how your parsed explanations performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    babbler.analyze(parses)\n",
    "except ValueError as e:\n",
    "    print(\"It seems as though none of your labeling functions were parsed. See the cells above and below for more information.\")\n",
    "    print(\"ERROR:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See which explanations were filtered and why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY\n",
      "4 TOTAL:\n",
      "1 Unparseable Explanation\n",
      "1 Duplicate Semantics\n",
      "0 Inconsistency with Example\n",
      "1 Uniform Signature\n",
      "1 Duplicate Signature\n",
      "0 Lowest Coverage\n",
      "\n",
      "[#1]: Unparseable Explanation\n",
      "\n",
      "Explanation: \n",
      "\n",
      "Reason: This explanation couldn't be parsed.\n",
      "\n",
      "Semantics: None\n",
      "\n",
      "\n",
      "[#2]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'self'.(= 'defense') else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because the word \"self\" occurs before \"defense\"\"\n",
      "\n",
      "Semantics: ('.root', ('.label', ('.int', 2), ('.call', ('.eq', ('.string', 'defense')), ('.string', 'self'))))\n",
      "\n",
      "\n",
      "[#3]: Uniform Signature\n",
      "\n",
      "Parse: return 1 if 'self'.(= 'defense') else 0\n",
      "\n",
      "Reason: This parse labeled NONE of the 400 development examples\n",
      "\n",
      "Semantics: ('.root', ('.label', ('.int', 2), ('.call', ('.eq', ('.string', 'defense')), ('.string', 'self'))))\n",
      "\n",
      "\n",
      "[#4]: Duplicate Signature\n",
      "\n",
      "Parse: return 1 if sum([s.startswith('electr') for s in [w for w in the word(s) the sentence]]).(>= 1) else 0\n",
      "\n",
      "Reason: This parse labeled identically to the following existing parse:\n",
      "\tExplanation(electr...: 1, \"A word in the sentence starts with \"electr\"\")\n",
      "\n",
      "Semantics: ('.root', ('.label', ('.int', 1), ('.call', ('.geq', ('.int', 1)), ('.sum', ('.map', ('.startswith', ('.string', 'electr')), ('.filter', ('.sentence',), 'words', '\\\\w+\\\\S*'))))))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "babbler.filtered_analysis(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 2 parse(s) from 2 explanations to set. (Total # parses = 2)\n",
      "\n",
      "Applying labeling functions to split 1\n",
      "[========================================] 100%\n",
      "\n",
      "Added 42 labels to split 1: L.nnz = 42, L.shape = (400, 2).\n",
      "Applying labeling functions to split 2\n",
      "[========================================] 100%\n",
      "\n",
      "Added 9 labels to split 2: L.nnz = 9, L.shape = (100, 2).\n"
     ]
    }
   ],
   "source": [
    "babbler.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Get feedback on the performance of your explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved label matrix for split 0: L.nnz = 52, L.shape = (477, 2)\n",
      "Retrieved label matrix for split 1: L.nnz = 42, L.shape = (400, 2)\n",
      "Retrieved label matrix for split 2: L.nnz = 9, L.shape = (100, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>electr..._0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>0.842105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politics_0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             j  Polarity  Coverage  Overlaps  Conflicts  Correct  Incorrect  \\\n",
       "electr..._0  0         1     0.095    0.0025     0.0025       32          6   \n",
       "politics_0   1         2     0.010    0.0025     0.0025        4          0   \n",
       "\n",
       "             Emp. Acc.  \n",
       "electr..._0   0.842105  \n",
       "politics_0    1.000000  "
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ls = [babbler.get_label_matrix(split) for split in [0,1,2]]\n",
    "lf_names = [lf.__name__ for lf in babbler.get_lfs()]\n",
    "lf_summary(Ls[1], Ys[1], lf_names=lf_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "[SUMMARY]\n",
      "Best model: [2]\n",
      "Best config: {'n_epochs': 100, 'show_plots': False, 'lr': 0.0011281415605346923, 'seed': 124}\n",
      "Best score: 0.693069306930693\n",
      "============================================================\n",
      "Precision: 0.530\n",
      "Recall: 1.000\n",
      "F1: 0.693\n"
     ]
    }
   ],
   "source": [
    "search_space = {\n",
    "    'n_epochs': [50, 100, 500],\n",
    "    'lr': {'range': [0.01, 0.001], 'scale': 'log'},\n",
    "    'show_plots': False,\n",
    "}\n",
    "\n",
    "tuner = RandomSearchTuner(LabelModel, seed=123)\n",
    "\n",
    "label_aggregator = tuner.search(\n",
    "    search_space, \n",
    "    train_args=[Ls[0]], \n",
    "    X_dev=Ls[1], Y_dev=Ys[1], \n",
    "    max_search=20, verbose=False, metric='f1')\n",
    "\n",
    "# record statistics over time\n",
    "pr, re, f1 = label_aggregator.score(Ls[1], Ys[1], metric=['precision', 'recall', 'f1'])\n",
    "stats = {\n",
    "    \"precision\": pr,\n",
    "    \"recall\": re,\n",
    "    \"f1\": f1,\n",
    "    \"time\": datetime.now(),\n",
    "    \"training_label_coverage\": label_coverage(Ls[0]),\n",
    "    \"training_label_size\": label_coverage(Ls[0])*len(dfs[0])\n",
    "}\n",
    "stat_history = stat_history.append(stats, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to save the explanations you've generated, you can use the `ExplanationIO` object to write to or read them from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'mention_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-297-d1e80f9ec695>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mFILE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"babbler_tutorial_explanations.tsv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mexp_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExplanationIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mexp_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexplanations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mexplanations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/snorkel-notebooks/babble/utils.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, explanations, fpath)\u001b[0m\n\u001b[1;32m     66\u001b[0m                     \u001b[0mcandidate_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                     \u001b[0mcandidate_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmention_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                 tsvwriter.writerow([\n\u001b[1;32m     70\u001b[0m                                     \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'mention_id'"
     ]
    }
   ],
   "source": [
    "stat_history.to_csv(\"babbler_tutorial_statistics_history.csv\")\n",
    "FILE = \"babbler_tutorial_explanations.tsv\"\n",
    "exp_io = ExplanationIO()\n",
    "exp_io.write(explanations, FILE)\n",
    "explanations = exp_io.read(FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "We can train a simple bag of words model on these labels, and see test accuracy.\n",
    "\n",
    "(This step may take a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_aggregator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-a38170fc1f22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_aggregator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'label_aggregator' is not defined"
     ]
    }
   ],
   "source": [
    "from analyzer import train_model\n",
    "\n",
    "train_model(label_aggregator, df_train, df_valid, df_test, L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
