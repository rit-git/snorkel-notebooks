{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if you need to unzip the data again.\n",
    "!ls ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the data if needed\n",
    "# Replace PASSWORD with the password to unzip\n",
    "\n",
    "!unzip -P PASSWORD ../data.zip -d ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from analyzer import load_dataset, update_stats, train_model, save_model\n",
    "from datetime import datetime\n",
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "from snorkel.labeling import LabelingFunction\n",
    "from snorkel.labeling import LabelModel\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling import PandasLFApplier\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_dev, df_valid, df_test = load_dataset(\"Amazon\")\n",
    "print(\"{} training examples\".format(len(df_train)))\n",
    "print(\"{} development examples\".format(len(df_dev)))\n",
    "print(\"{} validation examples\".format(len(df_valid)))\n",
    "print(\"{} test examples\".format(len(df_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the labels for this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSTAIN = -1\n",
    "NEGATIVE = 0\n",
    "POSITIVE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Customer Reviews Task\n",
    "\n",
    "### You will work with Amazon Customer Reviews, writing labeling functions that classify them as positive (1) or negative (0) sentiment. \n",
    "\n",
    "The reviews are from [Amazon](https://s3.amazonaws.com/amazon-reviews-pds/readme.html).\n",
    "All reviews were submitted with either 1 star (negative) or 5 star (positive) ratings. \n",
    "\n",
    "Your task is to __create labeling functions__ that take the text of the review as input, and output either a NEGATIVE or a POSITIVE or an ABSTAIN label. Try to write them as quickly and accurately as possible.\n",
    "\n",
    "You can evaluate your progress based on the coverage and f1 score of your label model, or by training a logistic regression classifier on the data and evaluating the test result.\n",
    "\n",
    "You may consult the internet at any time.\n",
    "\n",
    "Let's look at some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NEGATIVE\")\n",
    "negative_sample = df_dev[df_dev.label==NEGATIVE].sample(5)\n",
    "update_stats({\"examples\": negative_sample, \"class\": NEGATIVE}, \"show_examples\")\n",
    "display(negative_sample)\n",
    "\n",
    "print(\"POSITIVE\")\n",
    "positive_sample = df_dev[df_dev.label==POSITIVE].sample(5)\n",
    "update_stats({\"examples\": positive_sample, \"class\": POSITIVE}, \"show_examples\")\n",
    "display(positive_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Labeling Functions\n",
    "Time to write some labeling functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more imports or helper functions here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lf0(x):\n",
    "    if 'good' in x.text:\n",
    "        return POSITIVE\n",
    "    return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lf1(x):\n",
    "    return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lf2(x):\n",
    "    return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Test your function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "def test_func(lf, example_text):\n",
    "    x = SimpleNamespace(text=example_text)\n",
    "    update_stats({\"function\": lf.__name__, \"text\": example_text}, \"test_function\")\n",
    "    return lf(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_func(lf0, \"your text here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Functions\n",
    "This is how we obtain training labels, by training a model to combine the outputs of the noisy labeling functions.\n",
    "\n",
    "\n",
    "`L_train` and `L_dev` are matrices representing the label returned by each labeling function for each example in the training and development sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure all your functions are in this list!\n",
    "my_lfs = [lf0]\n",
    "\n",
    "\n",
    "lfs = [LabelingFunction(name=\"lf{}__\".format(i), f = lf ) for i, lf in enumerate(my_lfs)]\n",
    "\n",
    "# Apply the LFs to the unlabeled training data, and the development data\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_train = applier.apply(df_train)\n",
    "L_dev = applier.apply(df_dev)\n",
    "\n",
    "update_stats({\"num_lfs\": len(lfs), \"lfs\": lfs}, \"submit_lfs\", applier=applier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the snorkel model to combine these noisy labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the label model and compute the training labels\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train, n_epochs=500, log_freq=50, seed=123)\n",
    "df_train[\"pred_label\"] = label_model.predict_proba(L=L_train)[:,0]\n",
    "probs_train = df_train[\"pred_label\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# record intermediate results\n",
    "# Don't worry about this code block, we just store some metrics to keep track of your progress.\n",
    "Y_dev = df_dev.label.values\n",
    "stats = label_model.score(L=L_dev, Y=Y_dev, metrics=[\"f1\", \"precision\", \"recall\"])\n",
    "df_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(\n",
    "            X=df_train, y=probs_train, L=L_train)\n",
    "stats[\"training_label_coverage\"] = len(probs_train_filtered)/len(probs_train)\n",
    "stats[\"training_label_size\"] = len(probs_train_filtered)\n",
    "stats[\"num_lfs\"] = len(lfs)\n",
    "stats[\"data\"] = \"dev\"\n",
    "\n",
    "update_stats(stats, \"stats\", label_model=label_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see some examples of aggregated (probabilistic) labels!\n",
    "# re run this cell for new examples\n",
    "\n",
    "sample = df_train.sample(5)\n",
    "update_stats({\"examples\": sample, \"label\": \"label_model_predictions\"}, \"show_examples\")\n",
    "display(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Unlabeled Examples\n",
    "You can use these to brainstorm new labeling functions. You may try filtering or sorting them in other ways.\n",
    "\n",
    "If you get a `ValueError: a must be greater than 0 unless no samples are taken`, this means all your training examples are labeled by at least one LF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can filter for unlabeled data\n",
    "try:\n",
    "    df_unlabeled = df_train[~df_train.index.isin(df_train_filtered.index)]\n",
    "    sample = df_unlabeled.sample(5)\n",
    "    update_stats({\"examples\": sample, \"label\": \"unlabeled\"}, \"show_examples\")\n",
    "    display(sample)\n",
    "except ValueError as e:\n",
    "    print(\"ValueError: \")\n",
    "    print(e)\n",
    "    label_sums = (L_train != -1).sum(axis=1)\n",
    "    print(\"\\nExamples with lowest coverage: ({})\".format(min(label_sums)))\n",
    "    display(df_train[label_sums == min(label_sums)].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "Evaluate the accuracy of the estimated training labels and development set labels (based on ground truth).\n",
    "\n",
    "`Polarity` describes the set of outputs of each function, not including `ABSTAIN (-1)`.\n",
    "For example, a function that returns `ABSTAIN` or `POSITIVE` has polarity `[1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_analysis = LFAnalysis(L=L_train, lfs=lfs).lf_summary()\n",
    "display(\"Training set results:\", train_analysis)\n",
    "train_analysis['data'] = \"train\"\n",
    "train_analysis[\"lf_id\"] = train_analysis.index\n",
    "\n",
    "update_stats({\"num_lfs\": len(lfs), \"lf_ids\": train_analysis.j.tolist(), \"data\": \"train\"}, \"lf_analysis_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_dev = df_dev.label.values\n",
    "dev_analysis = LFAnalysis(L=L_dev, lfs=lfs).lf_summary(Y=Y_dev)\n",
    "display(\"Dev set results:\", dev_analysis)\n",
    "dev_analysis['data'] = \"dev\"\n",
    "dev_analysis[\"lf_id\"] = dev_analysis.index\n",
    "update_stats({\"num_lfs\": len(lfs), \"lf_ids\": dev_analysis.j.tolist(), \"data\": \"dev\"}, \"lf_analysis_dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Incorrectly Labeled Examples\n",
    "\n",
    "__FOR ONE (1) GIVEN LABELING FUNCTION__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view some incorrectly labeled examples for a given LF\n",
    "j = 0\n",
    "# set j to match the value of the LF you're interested in\n",
    "display(df_dev[L_dev[:,j]==abs(df_dev[\"label\"]-1)])\n",
    "update_stats({\"examples\": sample, \"label\": \"incorrect\", \"lf_id\": j}, \"show_examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "We can train a simple bag of words model on these labels to see the test accuracy.\n",
    "\n",
    "(This step may take a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = train_model(label_model, L_train)\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "When you have finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your name (for file naming)\n",
    "YOUR_NAME = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir snorkel_amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_model.save(\"snorkel_amazon/lfmodel.pkl\")\n",
    "%history -p -o -f snorkel_amazon/history.log\n",
    "!cp snorkel_amazon_task.ipynb snorkel_amazon/notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(YOUR_NAME) != 0\n",
    "save_model(YOUR_NAME, \"Snorkel\", \"Amazon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
