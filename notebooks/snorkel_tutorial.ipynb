{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the data\n",
    "# Replace PASSWORD with the password to unzip\n",
    "\n",
    "!unzip -P PASSWORD ../data.zip -d ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from analyzer import load_dataset, update_stats, train_model, save_model\n",
    "from datetime import datetime\n",
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "from snorkel.labeling import LabelingFunction\n",
    "from snorkel.labeling import LabelModel\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling import PandasLFApplier\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_dev, df_valid, df_test = load_dataset(\"News\")\n",
    "print(\"{} training examples\".format(len(df_train)))\n",
    "print(\"{} development examples\".format(len(df_dev)))\n",
    "print(\"{} validation examples\".format(len(df_valid)))\n",
    "print(\"{} test examples\".format(len(df_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snorkel Tutorial\n",
    "\n",
    "## News forum classification\n",
    "\n",
    "### You will work with a subset of the 20 NewsGroup dataset. \n",
    "The texts shown are from one of two forums:\n",
    " 1. Computer Electronics (Label 0)\n",
    " 2. Gun Politics Forum (Label 1)\n",
    "Your job is to create a training data set to classify texts as belonging to one of these two forums.\n",
    "\n",
    "You will do this by writing labeling functions mapping text to 0 (ELECTRONICS), 1 (GUNS), or -1 (ABSTAIN or no label).\n",
    "These functions will be aggregated by Snorkel to create training data from unlabeled examples.\n",
    "\n",
    "You can evaluate your progress based on the coverage and f1 score of your label model, or by training a logistic regression classifier on the data and evaluating the test result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the labels for this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSTAIN = -1\n",
    "ELECTRONICS = 0\n",
    "GUNS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ready to Roll\n",
    "Let's look at some positive and negative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun this cell to get a new sample\n",
    "print(\"ELECTRONICS\")\n",
    "sample = df_dev[df_dev.label==ELECTRONICS].sample(5)\n",
    "display(sample)\n",
    "update_stats({\"examples\": sample, \"class\": ELECTRONICS}, \"show_examples\")\n",
    "\n",
    "print(\"GUNS\")\n",
    "sample = df_dev[df_dev.label==GUNS].sample(5)\n",
    "display(sample)\n",
    "update_stats({\"examples\": sample, \"class\": GUNS}, \"show_examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Labeling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your task for this tutorial is to write 5 labeling functions.__\n",
    "\n",
    "Feel free to consult the internet or ask your experiment leader.\n",
    "\n",
    "*(For the real task, you will be given 30 minutes. You will still be allowed to use the internet in this phase, but not ask your experiment leader.)*\n",
    "\n",
    "Your function should take x as an input and output ELECTRONICS, GUNS, or ABSTAIN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lf0(x):\n",
    "    return GUNS if \"firearm\" in x.text.lower() else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your turn! try writing a function or editing the one above.\n",
    "\n",
    "If you want, you can write helper functions to reuse. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example helper function \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "def sentiment(text):\n",
    "    return sia.polarity_scores(text)[\"compound\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lf1(x):\n",
    "    return ELECTRONICS if \"tesla\" in x.text.lower() else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lf2(x):\n",
    "    count = len(re.findall(\"power\", x.text.lower())) \n",
    "    return ELECTRONICS if count > 2 else GUNS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lf3(x):\n",
    "    return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lf4(x):\n",
    "    return ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "def test_func(lf, example_text):\n",
    "    x = SimpleNamespace(text=example_text)\n",
    "    update_stats({\"function\": lf.__name__, \"text\": example_text}, \"test_function\")\n",
    "    return lf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_func(lf2, \"I've got a powerful powerful tesla coil\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Functions\n",
    "This is how we obtain training labels, by training a model to combine the outputs of the noisy labeling functions.\n",
    "`L_train` and `L_dev` are matrices representing the label returned by each labeling function for each example in the training and development sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure all your functions are in this list!\n",
    "my_lfs = [lf0]\n",
    "\n",
    "\n",
    "lfs = [LabelingFunction(name=\"lf{}__\".format(i), f = lf ) for i, lf in enumerate(my_lfs)]\n",
    "\n",
    "# Apply the LFs to the unlabeled training data, and the development data\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_train = applier.apply(df_train)\n",
    "L_dev = applier.apply(df_dev)\n",
    "\n",
    "update_stats({\"num_lfs\": len(lfs), \"lfs\": lfs}, \"submit_lfs\", applier=applier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the snorkel model to combine these noisy labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the label model and compute the training labels\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train, n_epochs=500, log_freq=50, seed=123)\n",
    "df_train[\"pred_label\"] = label_model.predict_proba(L=L_train)[:,0]\n",
    "probs_train = df_train[\"pred_label\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# record intermediate results\n",
    "# Don't worry about this code block, we just store some metrics to keep track of your progress.\n",
    "Y_dev = df_dev.label.values\n",
    "stats = label_model.score(L=L_dev, Y=Y_dev, metrics=[\"f1\", \"precision\", \"recall\"])\n",
    "df_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(\n",
    "            X=df_train, y=probs_train, L=L_train)\n",
    "stats[\"training_label_coverage\"] = len(probs_train_filtered)/len(probs_train)\n",
    "stats[\"training_label_size\"] = len(probs_train_filtered)\n",
    "stats[\"num_lfs\"] = len(lfs)\n",
    "stats[\"data\"] = \"dev\"\n",
    "\n",
    "update_stats(stats, \"stats\", label_model=label_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see some examples of aggregated (probabilistic) labels!\n",
    "# re run this cell for new examples\n",
    "\n",
    "sample = df_train.sample(5)\n",
    "update_stats({\"examples\": sample, \"label\": \"label_model_predictions\"}, \"show_examples\")\n",
    "display(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Unlabeled Examples\n",
    "You can use these to brainstorm new labeling functions. You may try filtering or sorting them in other ways.\n",
    "\n",
    "If you get a `ValueError: a must be greater than 0 unless no samples are taken`, this means all your training examples are labeled by at least one LF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can filter for unlabeled data\n",
    "try:\n",
    "    df_unlabeled = df_train[~df_train.index.isin(df_train_filtered.index)]\n",
    "    sample = df_unlabeled.sample(5)\n",
    "    update_stats({\"examples\": sample, \"label\": \"unlabeled\"}, \"show_examples\")\n",
    "    display(sample)\n",
    "except ValueError as e:\n",
    "    print(\"ValueError: \")\n",
    "    print(e)\n",
    "    label_sums = (L_train != -1).sum(axis=1)\n",
    "    print(\"\\nExamples with lowest coverage: ({})\".format(min(label_sums)))\n",
    "    display(df_train[label_sums == min(label_sums)].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "Evaluate the accuracy of the estimated training labels and development set labels (based on ground truth).\n",
    "\n",
    "`Polarity` describes the set of outputs of each function, not including `ABSTAIN (-1)`.\n",
    "For example, a function that returns `ABSTAIN` or `GUNS` has polarity `[1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_analysis = LFAnalysis(L=L_train, lfs=lfs).lf_summary()\n",
    "display(\"Training set results:\", train_analysis)\n",
    "train_analysis['data'] = \"train\"\n",
    "train_analysis[\"lf_id\"] = train_analysis.index\n",
    "\n",
    "update_stats({\"num_lfs\": len(lfs), \"lf_ids\": train_analysis.j.tolist(), \"data\": \"train\"}, \"lf_analysis_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_dev = df_dev.label.values\n",
    "dev_analysis = LFAnalysis(L=L_dev, lfs=lfs).lf_summary(Y=Y_dev)\n",
    "display(\"Dev set results:\", dev_analysis)\n",
    "dev_analysis['data'] = \"dev\"\n",
    "dev_analysis[\"lf_id\"] = dev_analysis.index\n",
    "update_stats({\"num_lfs\": len(lfs), \"lf_ids\": dev_analysis.j.tolist(), \"data\": \"dev\"}, \"lf_analysis_dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Incorrectly Labeled Examples\n",
    "\n",
    "__FOR ONE (1) GIVEN LABELING FUNCTION__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view some incorrectly labeled examples for a given LF\n",
    "j = 0\n",
    "# set j to match the value of the LF you're interested in\n",
    "display(df_dev[L_dev[:,j]==abs(df_dev[\"label\"]-1)])\n",
    "update_stats({\"examples\": sample, \"label\": \"incorrect\", \"lf_id\": j}, \"show_examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "We can train a simple bag of words model on these labels, and see test accuracy.\n",
    "\n",
    "(This step may take a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = train_model(label_model, L_train)\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## FINISHED?\n",
    "\n",
    "### It's time to save.\n",
    "\n",
    "When your time is up, please save your explanations and model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your name (for file naming)\n",
    "YOUR_NAME = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir snorkel_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_model.save(\"snorkel_tutorial/lfmodel.pkl\")\n",
    "%history -p -o -f snorkel_tutorial/history.log\n",
    "!cp snorkel_tutorial.ipynb snorkel_tutorial/notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert len(YOUR_NAME) != 0\n",
    "save_model(YOUR_NAME, \"Snorkel\", \"News\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...And you're done with the tutorial! \n",
    "\n",
    "## THANK YOU :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
