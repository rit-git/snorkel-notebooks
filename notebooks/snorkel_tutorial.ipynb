{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the necessary data and libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the data\n",
    "# Replace PASSWORD with the password to unzip\n",
    "\n",
    "!unzip -P PASSWORD ../data.zip -d ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from analyzer import load_dataset, update_stats, train_model, save_model\n",
    "from datetime import datetime\n",
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "from snorkel.labeling import LabelingFunction\n",
    "from snorkel.labeling import LabelModel\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling import PandasLFApplier\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_dev, df_valid, df_test = load_dataset(\"News\")\n",
    "print(\"{} training examples\".format(len(df_train)))\n",
    "print(\"{} development examples\".format(len(df_dev)))\n",
    "print(\"{} validation examples\".format(len(df_valid)))\n",
    "print(\"{} test examples\".format(len(df_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snorkel Tutorial\n",
    "\n",
    "## News forum classification\n",
    "\n",
    "### You will work with a subset of the 20 NewsGroup dataset. \n",
    "The texts shown are from one of two forums:\n",
    " 1. Computer Electronics (Label 0)\n",
    " 2. Gun Politics Forum (Label 1)\n",
    "Your job is to create a training data set to classify texts as belonging to one of these two forums.\n",
    "\n",
    "You will do this by writing labeling functions mapping text to 0 (ELECTRONICS), 1 (GUNS), or -1 (ABSTAIN or no label).\n",
    "These functions will be aggregated by Snorkel to create training data from unlabeled examples.\n",
    "\n",
    "You can evaluate your progress based on the coverage and f1 score of your label model, or by training a logistic regression classifier on the data and evaluating the test result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the labels for this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSTAIN = -1\n",
    "ELECTRONICS = 0\n",
    "GUNS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See Some Examples\n",
    "Let's look at some positive and negative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun this cell to get a new sample\n",
    "print(\"ELECTRONICS\")\n",
    "sample = df_dev[df_dev.label==ELECTRONICS].sample(5)\n",
    "display(sample)\n",
    "update_stats({\"examples\": sample, \"class\": ELECTRONICS}, \"show_examples\")\n",
    "\n",
    "print(\"GUNS\")\n",
    "sample = df_dev[df_dev.label==GUNS].sample(5)\n",
    "display(sample)\n",
    "update_stats({\"examples\": sample, \"class\": GUNS}, \"show_examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Labeling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to consult the internet or ask your experiment leader.\n",
    "\n",
    "*(For the real task, you will be given 30 minutes. You will still be allowed to use the internet in this phase, but not ask your experiment leader.)*\n",
    "\n",
    "Your function should take x as an input and output ELECTRONICS, GUNS, or ABSTAIN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lf0(x):\n",
    "    return GUNS if \"firearm\" in x.text.lower() else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want, you can write helper functions to reuse. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example helper function \n",
    "# see what ratio of a text is capitalized\n",
    "def ratio_all_caps(text):\n",
    "    x_chars = ''.join(text.split())  # remove all whitespace\n",
    "    if len(x_chars)==0:\n",
    "        return 0\n",
    "    x_upper = sum(i.isupper() for i in x_chars) / len(x_chars)\n",
    "    return x_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lf1(x):\n",
    "    if ratio_all_caps(x.text) > 0.8:\n",
    "        return GUNS\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lf2(x):\n",
    "    count = len(re.findall(\"power\", x.text.lower())) \n",
    "    if count >= 2:\n",
    "        return ELECTRONICS  \n",
    "    else: \n",
    "        return GUNS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lf3(x):\n",
    "    if \"tesla\" in x.text.lower():\n",
    "        return ELECTRONICS  \n",
    "    else: \n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lf4(x):\n",
    "    return ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your function (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "def test_func(lf, example_text):\n",
    "    x = SimpleNamespace(text=example_text)\n",
    "    update_stats({\"function\": lf.__name__, \"text\": example_text}, \"test_function\")\n",
    "    return lf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_func(lf2, \"I've got a powerful powerful tesla coil\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Functions\n",
    "This is how we obtain training labels, by training a model to combine the outputs of the noisy labeling functions.\n",
    "`L_train` and `L_dev` are matrices representing the label returned by each labeling function for each example in the training and development sets.\n",
    "\n",
    "You need to apply your functions each time to update them to see your results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure all your functions are in this list!\n",
    "my_lfs = [lf0, lf1, lf2, lf3, lf4]\n",
    "\n",
    "\n",
    "lfs = [LabelingFunction(name=\"lf{}__\".format(i), f = lf ) for i, lf in enumerate(my_lfs)]\n",
    "\n",
    "# Apply the LFs to the unlabeled training data, and the development data\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_train = applier.apply(df_train)\n",
    "L_dev = applier.apply(df_dev)\n",
    "\n",
    "update_stats({\"num_lfs\": len(lfs), \"lfs\": lfs}, \"submit_lfs\", applier=applier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the snorkel model to combine these noisy labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the label model and compute the training labels\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train, n_epochs=500, log_freq=50, seed=123)\n",
    "df_train[\"pred_label\"] = label_model.predict_proba(L=L_train)[:,0]\n",
    "df_dev[\"pred_label\"] = label_model.predict_proba(L=L_dev)[:,0]\n",
    "probs_train = df_train[\"pred_label\"]\n",
    "\n",
    "# record intermediate results\n",
    "# Don't worry about this code block, we just store some metrics to keep track of your progress.\n",
    "Y_dev = df_dev.label.values\n",
    "stats = label_model.score(L=L_dev, Y=Y_dev, metrics=[\"f1\", \"precision\", \"recall\"])\n",
    "df_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(\n",
    "            X=df_train, y=probs_train, L=L_train)\n",
    "stats[\"training_label_coverage\"] = len(probs_train_filtered)/len(probs_train)\n",
    "stats[\"training_label_size\"] = len(probs_train_filtered)\n",
    "stats[\"num_lfs\"] = len(lfs)\n",
    "stats[\"data\"] = \"dev\"\n",
    "\n",
    "display(stats)\n",
    "update_stats(stats, \"stats\", label_model=label_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stats from the output above tell you how good your training data is-- but there are a lot more questions you might have to figure out how to improve. Below are some tools at your disposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Analysis Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Probabilistic Label Output\n",
    "\n",
    "These are the labels created by your aggregated labelmodel, which will be used as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see some examples of aggregated (probabilistic) labels!\n",
    "# re run this cell for new examples\n",
    "\n",
    "sample = df_train.sample(5)\n",
    "update_stats({\"examples\": sample, \"label\": \"label_model_predictions\"}, \"show_examples\")\n",
    "display(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See Performance of Each LF\n",
    "\n",
    "Evaluate the accuracy of the estimated training labels and development set labels (based on ground truth).\n",
    "\n",
    "`Polarity` describes the set of outputs of each function, not including `ABSTAIN (-1)`.\n",
    "For example, a function that returns `ABSTAIN` or `GUNS` has polarity `[1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_analysis = LFAnalysis(L=L_train, lfs=lfs).lf_summary()\n",
    "display(\"Training set results:\", train_analysis)\n",
    "train_analysis['data'] = \"train\"\n",
    "train_analysis[\"lf_id\"] = train_analysis.index\n",
    "\n",
    "update_stats({\"num_lfs\": len(lfs), \"lf_ids\": train_analysis.j.tolist(), \"data\": \"train\"}, \"lf_analysis_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_dev = df_dev.label.values\n",
    "dev_analysis = LFAnalysis(L=L_dev, lfs=lfs).lf_summary(Y=Y_dev)\n",
    "display(\"Dev set results:\", dev_analysis)\n",
    "dev_analysis['data'] = \"dev\"\n",
    "dev_analysis[\"lf_id\"] = dev_analysis.index\n",
    "update_stats({\"num_lfs\": len(lfs), \"lf_ids\": dev_analysis.j.tolist(), \"data\": \"dev\"}, \"lf_analysis_dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Unlabeled/Least Labeled Examples\n",
    "You can use these to brainstorm new labeling functions. You may try filtering or sorting them in other ways.\n",
    "\n",
    "If you see \"All Examples are Labeled\", this means all your training examples are labeled by at least one LF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can filter for unlabeled data\n",
    "try:\n",
    "    df_unlabeled = df_train[~df_train.index.isin(df_train_filtered.index)]\n",
    "    sample = df_unlabeled.sample(5)\n",
    "    update_stats({\"examples\": sample, \"label\": \"unlabeled\"}, \"show_examples\")\n",
    "    display(sample[[\"text\", \"pred_label\"]])\n",
    "except ValueError as e:\n",
    "    print(\"All examples are labeled. Showing Lowest Coverage examples.\")\n",
    "    label_sums = (L_train != -1).sum(axis=1)\n",
    "    print(\"\\nExamples with lowest coverage: ({})\".format(min(label_sums)))\n",
    "    sample = df_train[label_sums == min(label_sums)].head()\n",
    "    update_stats({\"examples\": sample, \"label\": \"unlabeled\"}, \"show_examples\")\n",
    "    display(sample[[\"text\", \"pred_label\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Incorrectly Labeled Examples\n",
    "\n",
    "__FOR ONE (1) GIVEN LABELING FUNCTION__ see some examples where it underperforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view some incorrectly labeled examples for a given LF\n",
    "j = 2\n",
    "# set j to match the value of the LF you're interested in\n",
    "df_dev[\"label_from_this_LF\"] = L_dev[:,j]\n",
    "sample = df_dev[L_dev[:,j]==abs(df_dev[\"label\"]-1)]\n",
    "display(sample)\n",
    "update_stats({\"examples\": sample, \"label\": \"incorrect\", \"lf_id\": j}, \"show_examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "\n",
    "The size and accuracy of the training data isn't really enough to know how \"good\" the training data is. \n",
    "\n",
    "For this, we can train a simple bag of words model on these labels, and see test accuracy.\n",
    "\n",
    "(This step may take a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = train_model(label_model, L_train)\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## FINISHED?\n",
    "\n",
    "### It's time to save.\n",
    "\n",
    "When your time is up, please save your explanations and model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your name (for file naming)\n",
    "YOUR_NAME = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir snorkel_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_model.save(\"snorkel_tutorial/lfmodel.pkl\")\n",
    "%history -p -o -f snorkel_tutorial/history.log\n",
    "!cp snorkel_tutorial.ipynb snorkel_tutorial/notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert len(YOUR_NAME) != 0\n",
    "save_model(YOUR_NAME, \"Snorkel\", \"News\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...And you're done with the tutorial! \n",
    "\n",
    "## THANK YOU :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
