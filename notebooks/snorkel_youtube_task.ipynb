{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if you need to unzip the data again.\n",
    "!ls ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the data if needed\n",
    "# Replace PASSWORD with the password to unzip\n",
    "\n",
    "!unzip -P PASSWORD ../data.zip -d ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from analyzer import load_dataset, update_stats, train_model, save_model\n",
    "from datetime import datetime\n",
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "from snorkel.labeling import LabelingFunction\n",
    "from snorkel.labeling import LabelModel\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling import PandasLFApplier\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_dev, df_valid, df_test = load_dataset(\"Youtube\")\n",
    "print(\"{} training examples\".format(len(df_train)))\n",
    "print(\"{} development examples\".format(len(df_dev)))\n",
    "print(\"{} validation examples\".format(len(df_valid)))\n",
    "print(\"{} test examples\".format(len(df_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube Spam Classification Task\n",
    "\n",
    "### For this task, you will work with comments from 5 different YouTube videos, and classify comments as either spam (1) or legitimate comments (0) by writing labeling functions.\n",
    "\n",
    "Spam can be defined as irrelevant or unsolicited messages sent over the Internet.\n",
    "\n",
    "The data is optained [from Kaggle](https://www.kaggle.com/goneee/youtube-spam-classifiedcomments). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define variable names for the labels in this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define labels\n",
    "ABSTAIN = -1\n",
    "NOT_SPAM = 0\n",
    "SPAM = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See Some Examples\n",
    "Let's see some examples from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Some labeled examples: \")\n",
    "print(\"NOT SPAM\")\n",
    "sample = df_dev[df_dev.label==NOT_SPAM].sample(5)\n",
    "display(sample)\n",
    "update_stats({\"examples\": sample, \"class\": NOT_SPAM}, \"show_examples\")\n",
    "\n",
    "print(\"SPAM\")\n",
    "sample = df_dev[df_dev.label==SPAM].sample(5)\n",
    "display(sample)\n",
    "update_stats({\"examples\": sample, \"class\": SPAM}, \"show_examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Labeling Functions\n",
    "\n",
    "Time to write some labeling functions! \n",
    "\n",
    "Your task is to __create labeling functions__ that take the text of the review as input, and output either a SPAM or a NOT_SPAM or an ABSTAIN label. Try to write them as quickly and accurately as possible.\n",
    "\n",
    "You may consult the internet at any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lf0(x):\n",
    "    return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lf1(x):\n",
    "    return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lf2(x):\n",
    "    return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Test your function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "def test_func(lf, example_text):\n",
    "    x = SimpleNamespace(text=example_text)\n",
    "    update_stats({\"function\": lf.__name__, \"text\": example_text}, \"test_function\")\n",
    "    return lf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_func(lf0, \"your text here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Functions\n",
    "This is how we obtain training labels, by training a model to combine the outputs of the noisy labeling functions.\n",
    "`L_train` and `L_dev` are matrices representing the label returned by each labeling function for each example in the training and development sets.\n",
    "\n",
    "You need to apply your functions each time to update them to see your results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure all your functions are in this list!\n",
    "my_lfs = [lf0]\n",
    "\n",
    "\n",
    "lfs = [LabelingFunction(name=\"lf{}__\".format(i), f = lf ) for i, lf in enumerate(my_lfs)]\n",
    "\n",
    "# Apply the LFs to the unlabeled training data, and the development data\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_train = applier.apply(df_train)\n",
    "L_dev = applier.apply(df_dev)\n",
    "\n",
    "update_stats({\"num_lfs\": len(lfs), \"lfs\": lfs}, \"submit_lfs\", applier=applier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the label model and compute the training labels\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train, n_epochs=500, log_freq=50, seed=123)\n",
    "df_train[\"pred_label\"] = label_model.predict_proba(L=L_train)[:,0]\n",
    "df_dev[\"pred_label\"] = label_model.predict_proba(L=L_dev)[:,0]\n",
    "\n",
    "probs_train = df_train[\"pred_label\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# record intermediate results\n",
    "# Don't worry about this code block, we just store some metrics to keep track of your progress.\n",
    "Y_dev = df_dev.label.values\n",
    "stats = label_model.score(L=L_dev, Y=Y_dev, metrics=[\"f1\", \"precision\", \"recall\"])\n",
    "df_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(\n",
    "            X=df_train, y=probs_train, L=L_train)\n",
    "stats[\"training_label_coverage\"] = len(probs_train_filtered)/len(probs_train)\n",
    "stats[\"training_label_size\"] = len(probs_train_filtered)\n",
    "stats[\"num_lfs\"] = len(lfs)\n",
    "stats[\"data\"] = \"dev\"\n",
    "\n",
    "display(stats)\n",
    "update_stats(stats, \"stats\", label_model=label_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Analysis Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Probabilistic Label Output\n",
    "\n",
    "These are the labels created by your aggregated labelmodel, which will be used as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see some examples of aggregated (probabilistic) labels!\n",
    "# re run this cell for new examples\n",
    "\n",
    "sample = df_train.sample(5)\n",
    "update_stats({\"examples\": sample, \"label\": \"label_model_predictions\"}, \"show_examples\")\n",
    "display(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See Performance of Each LF\n",
    "Evaluate the accuracy of the estimated training labels and development set labels (based on ground truth).\n",
    "\n",
    "`Polarity` describes the set of outputs of each function, not including `ABSTAIN (-1)`.\n",
    "For example, a function that returns `ABSTAIN` or `SPAM` has polarity `[1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_analysis = LFAnalysis(L=L_train, lfs=lfs).lf_summary()\n",
    "display(\"Training set results:\", train_analysis)\n",
    "train_analysis['data'] = \"train\"\n",
    "train_analysis[\"lf_id\"] = train_analysis.index\n",
    "\n",
    "update_stats({\"num_lfs\": len(lfs), \"lf_ids\": train_analysis.j.tolist(), \"data\": \"train\"}, \"lf_analysis_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_dev = df_dev.label.values\n",
    "dev_analysis = LFAnalysis(L=L_dev, lfs=lfs).lf_summary(Y=Y_dev)\n",
    "display(\"Dev set results:\", dev_analysis)\n",
    "dev_analysis['data'] = \"dev\"\n",
    "dev_analysis[\"lf_id\"] = dev_analysis.index\n",
    "update_stats({\"num_lfs\": len(lfs), \"lf_ids\": dev_analysis.j.tolist(), \"data\": \"dev\"}, \"lf_analysis_dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Unlabeled/Least Labeled Examples\n",
    "You can use these to brainstorm new labeling functions. You may try filtering or sorting them in other ways.\n",
    "\n",
    "If you see \"All Examples are Labeled\", this means all your training examples are labeled by at least one LF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can filter for unlabeled data\n",
    "try:\n",
    "    df_unlabeled = df_train[~df_train.index.isin(df_train_filtered.index)]\n",
    "    sample = df_unlabeled.sample(5)\n",
    "    update_stats({\"examples\": sample, \"label\": \"unlabeled\"}, \"show_examples\")\n",
    "    display(sample)\n",
    "except ValueError as e:\n",
    "    print(\"ValueError: \")\n",
    "    print(e)\n",
    "    label_sums = (L_train != -1).sum(axis=1)\n",
    "    print(\"\\nExamples with lowest coverage: ({})\".format(min(label_sums)))\n",
    "    display(df_train[label_sums == min(label_sums)].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Incorrectly Labeled Examples\n",
    "\n",
    "__FOR ONE (1) GIVEN LABELING FUNCTION__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view some incorrectly labeled examples for a given LF\n",
    "j = 2\n",
    "# set j to match the value of the LF you're interested in\n",
    "df_dev[\"label_from_this_LF\"] = L_dev[:,j]\n",
    "sample = df_dev[L_dev[:,j]==abs(df_dev[\"label\"]-1)]\n",
    "display(sample)\n",
    "update_stats({\"examples\": sample, \"label\": \"incorrect\", \"lf_id\": j}, \"show_examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "We can train a simple bag of words model on these labels to see the test accuracy.\n",
    "\n",
    "(This step may take a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = train_model(label_model, L_train)\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "When you have finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your name (for file naming)\n",
    "YOUR_NAME = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir snorkel_youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_model.save(\"snorkel_youtube/lfmodel.pkl\")\n",
    "%history -p -o -f snorkel_youtube/history.log\n",
    "!cp snorkel_youtube_task.ipynb snorkel_youtube/notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(YOUR_NAME) != 0\n",
    "save_model(YOUR_NAME, \"Snorkel\", \"Youtube\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
