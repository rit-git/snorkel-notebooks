{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube Spam Classification Task\n",
    "\n",
    "### For this task, you will work with comments from 5 different YouTube videos, and classify comments as either spam (1) or legitimate comments (0) by writing labeling functions.\n",
    "\n",
    "Spam can be defined as irrelevant or unsolicited messages sent over the Internet.\n",
    "\n",
    "First, import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.preparer import load_youtube_dataset\n",
    "from snorkel.labeling import labeling_function\n",
    "from snorkel.labeling import LabelModel\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling import PandasLFApplier\n",
    "from analyzer import train_model\n",
    "import re\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "The data is available [via Kaggle](https://www.kaggle.com/goneee/youtube-spam-classifiedcomments). You may download it there, or, if you have the password, unzip the data below.\n",
    "\n",
    "You must replace `PASSWORD` with the password to unzip the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -P PASSWORD data/data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELIMITER = \"#\"\n",
    "df_train, df_dev, df_valid, df_test = load_youtube_dataset(delimiter=DELIMITER)\n",
    "print(\"{} training examples\".format(len(df_train)))\n",
    "print(\"{} development examples\".format(len(df_dev)))\n",
    "print(\"{} validation examples\".format(len(df_valid)))\n",
    "print(\"{} test examples\".format(len(df_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define variable names for the labels in this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define labels\n",
    "ABSTAIN = -1\n",
    "NOT_SPAM = 0\n",
    "SPAM = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some labeled examples: \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_dev' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5f12cd31a857>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some labeled examples: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_dev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_dev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mNOT_SPAM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_dev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_dev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mSPAM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_dev' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Some labeled examples: \")\n",
    "display(df_dev[df_dev.label==NOT_SPAM].sample(5))\n",
    "display(df_dev[df_dev.label==SPAM].sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Labeling Functions\n",
    "Time to write some labeling functions! Below is an example. Be sure to add your function to the list `lfs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def my_first_labeling_function(x):\n",
    "    return SPAM if \"my\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "lfs.append(my_first_labeling_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Functions\n",
    "This is how we obtain training labels, by training a model to combine the outputs of the noisy labeling functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the LFs to the unlabeled training data, and the development data\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_train = applier.apply(df_train)\n",
    "L_dev = applier.apply(df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the label model and compute the training labels\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train, n_epochs=500, log_freq=50, seed=123)\n",
    "df_train[\"label\"] = label_model.predict(L=L_train, tie_break_policy=\"abstain\")\n",
    "display(df_train.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Unlabeled Examples\n",
    "You can use these to brainstorm new labeling functions. You may try filtering or sorting them in other ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can filter for unlabeled data\n",
    "df_unlabeled = df_train[df_train.label == ABSTAIN]\n",
    "display(df_unlabeled.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "Evaluate the accuracy of the estimated training labels and development set labels (based on ground truth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = df_train.label.values\n",
    "train_analysis = LFAnalysis(L=L_train, lfs=lfs).lf_summary(Y=Y_train)\n",
    "display(\"Training set results:\", train_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_dev = df_dev.label.values\n",
    "dev_analysis = LFAnalysis(L=L_dev, lfs=lfs).lf_summary(Y=Y_dev)\n",
    "display(\"Dev set results:\", dev_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "Train a simple bag of words model on these labels, and report test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(label_model, df_train, df_valid, df_test, L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
