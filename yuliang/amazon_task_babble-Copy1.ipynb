{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nofarcarmeli/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from data.preparer import load_amazon_dataset\n",
    "\n",
    "from babble import Explanation\n",
    "from babble import BabbleStream\n",
    "from babble.Candidate import Candidate \n",
    "\n",
    "from metal.analysis import lf_summary\n",
    "from metal.analysis import label_coverage\n",
    "from metal import LabelModel\n",
    "from metal.tuners import RandomSearchTuner\n",
    "from babble.utils import ExplanationIO\n",
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "stat_history = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 training examples\n",
      "500 development examples\n",
      "500 validation examples\n",
      "1000 test examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nofarcarmeli/Documents/GitHub/snorkel-notebooks/venv/lib/python3.7/site-packages/pandas/core/frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "DELIMITER = \"#\"\n",
    "df_train, df_dev, df_valid, df_test = load_amazon_dataset(delimiter=DELIMITER)\n",
    "print(\"{} training examples\".format(len(df_train)))\n",
    "print(\"{} development examples\".format(len(df_dev)))\n",
    "print(\"{} validation examples\".format(len(df_valid)))\n",
    "print(\"{} test examples\".format(len(df_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the data into a format compatible with Babble Labble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_train.copy(), df_dev.copy()]\n",
    "dfs[0]['label'] = -1\n",
    "\n",
    "for df in dfs:\n",
    "    df[\"id\"] = range(len(df))\n",
    "    df[\"label\"] += 1\n",
    "\n",
    "Cs = [df.apply(lambda x: Candidate(x), axis=1) for df in dfs]\n",
    "\n",
    "# babble labble uses 1 and 2 for labels, while our data uses 0 and 1\n",
    "# add 1 to convert\n",
    "Ys = [df.label.values for df in dfs]\n",
    "Ys[0] -= 1 # no label (training set) should be set to -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Customer Reviews Classification with Babble\n",
    "\n",
    "### For this task, you will work with Amazon Customer Reviews, writing explanations about how to classify them as positive or negative sentiment.\n",
    "\n",
    "Only 1 star and 5 star reviews are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_lfs</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-02-03 16:16:09.843386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_lfs                       time\n",
       "0      0.0 2020-02-03 16:16:09.843386"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start timer!\n",
    "stat_history.append({\"time\": datetime.now(), \"num_lfs\": 0}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define labels\n",
    "ABSTAIN = 0\n",
    "NEGATIVE = 1\n",
    "POSITIVE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that aliases are a way to refer to a set of words in a rule. \n",
    "\n",
    "For example, with\n",
    "`aliases = {\"couples\": [\"girlfriend\", \"boyfriend\", \"wife\", \"husband\"]}` \n",
    "\n",
    "--> now you can refer to \"couples\" in a rule, and the parser will know you mean any of these terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar construction complete.\n"
     ]
    }
   ],
   "source": [
    "aliases = {\"positive_adj\": [\"good\", \"great\", \"nice\", \"awesome\", \"cool\", \"amazing\", \"funny\", \"enjoy\", \"enjoyable\", \"fun\", \"like\", \"touching\"],\n",
    "           \"negative_adj\": [\"bad\", \"worse\", \"worst\", \"terrible\", \"ugly\", \"boring\", \"dislike\", \"hate\", \"dissapointed\", \"however\"],\n",
    "           \"negation\": [\"not\", \"wasn\", \"weren\", \"isn\", \"aren\", \"don\", \"doesn\", \"didn\", \"no\"]} \n",
    "babbler = BabbleStream(Cs, Ys, balanced=True, shuffled=True, seed=456, aliases=aliases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettyprint(candidate):\n",
    "    # just a helper function to print the candidate nicely\n",
    "    print(candidate.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example candidate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After reading Shadow Watch i was dissapointed. Most Clancy books are great but this one however was just, well....not good. The beginning was ok with a space shuttle that blows up at its launch date and leaves everybody in the book (including you the reader) in shock. the book from there manages to keep up with an all out assault on a factory in SA. But from there the book dies. None of the characters were described well in this book and it was hard to finish the next 200 some pages. The only real reason this book was published was because Tom Clancy had his name on it. To make it short, spare yourself of this book and get a good Tom Clancy book such as Patriot Games or Red Storm Rising\n"
     ]
    }
   ],
   "source": [
    "candidate = babbler.next()\n",
    "prettyprint(candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Instructions\n",
    "\n",
    "All reviews were submitted with either 1 star (negative) or 5 star (positive) ratings. \n",
    "\n",
    "Your task is to __create labeling functions__ by writing natural language explanations of labeling rules. Try to write them as quickly and accurately as possible.\n",
    "\n",
    "You may consult the internet at any time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Explanations\n",
    "\n",
    "Creating explanations generally happens in five steps:\n",
    "1. View candidates\n",
    "2. Write explanations\n",
    "3. Get feedback\n",
    "4. Update explanations \n",
    "5. Apply label aggregator\n",
    "\n",
    "Steps 3-5 are optional; explanations may be submitted without any feedback on their quality. However, in our experience, observing how well explanations are being parsed and what their accuracy/coverage on a dev set are (if available) can quickly lead to simple improvements that yield significantly more useful labeling functions. Once a few labeling functions have been collected, you can use the label aggregator to identify candidates that are being mislabeled and write additional explanations targeting those failure modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection\n",
    "\n",
    "Use `babbler` to show candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I really enjoy Blackfield's second CD and hope they continue to record in the future. I think Blackfield II is better than the band's first disc (and that one was great). As Steven Wilson's other band (Porcupine Tree) continues to play harder rock, Blackfield seems to be his outlet for good pop rock and akin to Porcupine Tree's \"Stupid Dream\" and \"Lightbulb Sun\" era. Now I'm trying to decide if I prefer Porcupine Tree's brand new \"Fear of a Blank Planet\" disc or Blackfield II. I think I'll just enjoy them both!\n",
      "{'key': 185974, 'text': 'I really enjoy Blackfield\\'s second CD and hope they continue to record in the future. I think Blackfield II is better than the band\\'s first disc (and that one was great). As Steven Wilson\\'s other band (Porcupine Tree) continues to play harder rock, Blackfield seems to be his outlet for good pop rock and akin to Porcupine Tree\\'s \"Stupid Dream\" and \"Lightbulb Sun\" era. Now I\\'m trying to decide if I prefer Porcupine Tree\\'s brand new \"Fear of a Blank Planet\" disc or Blackfield II. I think I\\'ll just enjoy them both!', 'label': 2, 'id': 131, 'tokens': ['I', 'really', 'enjoy', 'Blackfield', \"'s\", 'second', 'CD', 'and', 'hope', 'they', 'continue', 'to', 'record', 'in', 'the', 'future', '.', 'I', 'think', 'Blackfield', 'II', 'is', 'better', 'than', 'the', 'band', \"'s\", 'first', 'disc', '(', 'and', 'that', 'one', 'was', 'great', ')', '.', 'As', 'Steven', 'Wilson', \"'s\", 'other', 'band', '(', 'Porcupine', 'Tree', ')', 'continues', 'to', 'play', 'harder', 'rock', ',', 'Blackfield', 'seems', 'to', 'be', 'his', 'outlet', 'for', 'good', 'pop', 'rock', 'and', 'akin', 'to', 'Porcupine', 'Tree', \"'s\", '``', 'Stupid', 'Dream', \"''\", 'and', '``', 'Lightbulb', 'Sun', \"''\", 'era', '.', 'Now', 'I', \"'m\", 'trying', 'to', 'decide', 'if', 'I', 'prefer', 'Porcupine', 'Tree', \"'s\", 'brand', 'new', '``', 'Fear', 'of', 'a', 'Blank', 'Planet', \"''\", 'disc', 'or', 'Blackfield', 'II', '.', 'I', 'think', 'I', \"'ll\", 'just', 'enjoy', 'them', 'both', '!'], 'doc_id': 131, 'mention_id': 131}\n",
      "131\n"
     ]
    }
   ],
   "source": [
    "candidate = babbler.next()\n",
    "prettyprint(candidate)\n",
    "print(candidate)\n",
    "print(candidate.mention_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't know whether it's positive or negative, it's okay to make your best guess or skip an example.\n",
    "For a candidate you decide to label, write an explanation of why you chose that label.\n",
    "\n",
    "You can consult the internet or refer to the babble tutorial notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "e0 = Explanation(\n",
    "    # feel free to change the name to something that describes your rule better.\n",
    "    name = \"e0\", \n",
    "    label = NEGATIVE, \n",
    "    condition = 'because the word \"terrible\" occurs', \n",
    "    # remember that is argument (candidate) is optional. \n",
    "    # You can use it to make sure the explanation applies to the candidate you pass as an argument.\n",
    "    candidate = 478\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = Explanation(\n",
    "    name = \"e1\", \n",
    "    label = POSITIVE, \n",
    "    condition = 'a word in the sentence is a positive_adj', \n",
    "    candidate = 486\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2 = Explanation(\n",
    "    name = \"positive_adj\", \n",
    "    label = POSITIVE, \n",
    "    condition = \"a word in the sentence is a positive_adj\", \n",
    "    candidate = 131\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "e3 = Explanation(\n",
    "    name = \"negative_adj\", \n",
    "    label = NEGATIVE, \n",
    "    condition = \"a word in the sentence is a negative_adj\", \n",
    "    candidate = 479\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "e4 = Explanation(\n",
    "    name = \"e4\", \n",
    "    label = NEGATIVE, \n",
    "    condition = \"a word in the sentence is a positive_adj after negation\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "e5 = Explanation(\n",
    "    name = \"e5\", \n",
    "    label = POSITIVE, \n",
    "    condition = \"a word in the sentence is a negative_adj after negation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "e6 = Explanation(\n",
    "    name = \"e6\", \n",
    "    label = POSITIVE, \n",
    "    condition = \"because synonyms of 'good' occurs\", \n",
    "    candidate = candidate.mention_id \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "e7 = Explanation(\n",
    "    name = \"e7\", \n",
    "    label = ABSTAIN, \n",
    "    condition = \"\", \n",
    "    candidate = candidate.mention_id \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "e8 = Explanation(\n",
    "    name = \"e8\", \n",
    "    label = ABSTAIN, \n",
    "    condition = \"\", \n",
    "    candidate = candidate.mention_id \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "e9 = Explanation(\n",
    "    name = \"e9\", \n",
    "    label = ABSTAIN, \n",
    "    condition = \"\", \n",
    "    candidate = candidate.mention_id \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "e10 = Explanation(\n",
    "    name = \"e10\", \n",
    "    label = ABSTAIN, \n",
    "    condition = \"\", \n",
    "    candidate = candidate.mention_id \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Babble will parse your explanations into functions, then filter out functions that are duplicates, incorrectly label their given candidate, or assign the same label to all examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building list of target candidate ids...\n",
      "Collected 4 unique target candidate ids from 10 explanations.\n",
      "Gathering desired candidates...\n",
      "Found 4/4 desired candidates\n",
      "Linking explanations to candidates...\n",
      "Linked 8/10 explanations\n",
      "5 explanation(s) out of 10 were parseable.\n",
      "14 parse(s) generated from 10 explanation(s).\n",
      "12 parse(s) remain (2 parse(s) removed by DuplicateSemanticsFilter).\n",
      "Note: 12 LFs did not have candidates and therefore could not be filtered.\n",
      "12 parse(s) remain (0 parse(s) removed by ConsistencyFilter).\n",
      "Applying labeling functions to investigate labeling signature.\n",
      "[==============================          ] 75%"
     ]
    }
   ],
   "source": [
    "# Add any explanations that you haven't committed yet\n",
    "explanations = [e0, e1, e2, e3, e4, e5, e6, e7, e8, e9]\n",
    "\n",
    "parses, filtered = babbler.apply(explanations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "See how your explanations were parsed and filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    babbler.analyze(parses)\n",
    "except ValueError as e:\n",
    "    print(\"It seems as though none of your labeling functions were parsed. See the cells above and below for more information.\")\n",
    "    print(\"ERROR:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "babbler.filtered_analysis(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "babbler.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Get feedback on the performance of your explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ls = [babbler.get_label_matrix(split) for split in [0,1,2]]\n",
    "lf_names = [lf.__name__ for lf in babbler.get_lfs()]\n",
    "lf_summary(Ls[1], Ys[1], lf_names=lf_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'n_epochs': [50, 100, 500],\n",
    "    'lr': {'range': [0.01, 0.001], 'scale': 'log'},\n",
    "    'show_plots': False,\n",
    "}\n",
    "\n",
    "tuner = RandomSearchTuner(LabelModel, seed=123)\n",
    "\n",
    "label_aggregator = tuner.search(\n",
    "    search_space, \n",
    "    train_args=[Ls[0]], \n",
    "    X_dev=Ls[1], Y_dev=Ys[1], \n",
    "    max_search=20, verbose=False, metric='f1')\n",
    "\n",
    "# record statistics over time\n",
    "pr, re, f1 = label_aggregator.score(Ls[1], Ys[1], metric=['precision', 'recall', 'f1'])\n",
    "stats = {\n",
    "    \"precision\": pr,\n",
    "    \"recall\": re,\n",
    "    \"f1\": f1,\n",
    "    \"time\": datetime.now(),\n",
    "    \"training_label_coverage\": label_coverage(Ls[0]),\n",
    "    \"training_label_size\": label_coverage(Ls[0])*len(dfs[0])\n",
    "}\n",
    "stat_history = stat_history.append(stats, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view some incorrectly labeled examples for a given LF\n",
    "j = 0\n",
    "print(lf_names[j])\n",
    "# set j to match the value of the LF you're interested in\n",
    "L_dev = Ls[1].todense()\n",
    "display(df_dev[L_dev[:,j].A1==abs(df_dev[\"label\"]-3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "We can train a simple bag of words model on these labels, and see test accuracy.\n",
    "\n",
    "(This step may take a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_train = Ls[0].todense()\n",
    "probs_train = label_aggregator.predict_proba(L=L_train)\n",
    "mask = (L_train != 0).any(axis=1).A1\n",
    "df_train_filtered = df_train.iloc[mask]\n",
    "probs_train_filtered = probs_train[mask]\n",
    "print(\"{} out of {} examples used for training data\".format(len(df_train_filtered), len(df_train)))\n",
    "\n",
    "from analyzer import train_model_from_probs\n",
    "\n",
    "for df in df_valid, df_test:\n",
    "    vc = df[\"label\"].value_counts()\n",
    "    assert len(vc) == 2\n",
    "    vc.iloc[0]\n",
    "    vc.iloc[1]\n",
    "\n",
    "train_model_from_probs(df_train_filtered, probs_train_filtered, df_valid, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "386128    0\n",
       "297944    1\n",
       "164196    0\n",
       "76962     0\n",
       "52185     1\n",
       "387208    0\n",
       "142735    1\n",
       "41715     0\n",
       "258897    1\n",
       "125129    0\n",
       "313407    0\n",
       "134859    0\n",
       "44102     0\n",
       "321308    0\n",
       "308179    1\n",
       "342248    0\n",
       "139644    1\n",
       "324172    0\n",
       "261967    0\n",
       "4530      0\n",
       "166087    0\n",
       "397161    0\n",
       "111380    1\n",
       "15494     0\n",
       "203610    0\n",
       "368848    1\n",
       "375145    1\n",
       "137134    0\n",
       "354772    0\n",
       "11561     1\n",
       "         ..\n",
       "77463     0\n",
       "299037    1\n",
       "282521    1\n",
       "49066     0\n",
       "190281    1\n",
       "292641    1\n",
       "220224    0\n",
       "93251     1\n",
       "94037     0\n",
       "184303    1\n",
       "318517    0\n",
       "63371     1\n",
       "199418    1\n",
       "312380    1\n",
       "266667    1\n",
       "9380      0\n",
       "251940    0\n",
       "136400    0\n",
       "172166    0\n",
       "101050    0\n",
       "283483    1\n",
       "287443    0\n",
       "203332    1\n",
       "176165    1\n",
       "248590    1\n",
       "166147    1\n",
       "195886    0\n",
       "93541     1\n",
       "269067    1\n",
       "160020    1\n",
       "Name: label, Length: 1000, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_logreg(input_dim, output_dim=2):\n",
    "    model = tf.keras.Sequential()\n",
    "    if output_dim == 1:\n",
    "        loss = \"binary_crossentropy\"\n",
    "        activation = tf.nn.sigmoid\n",
    "    else:\n",
    "        loss = \"categorical_crossentropy\"\n",
    "        activation = tf.nn.softmax\n",
    "    dense = tf.keras.layers.Dense(\n",
    "        units=output_dim,\n",
    "        input_dim=input_dim,\n",
    "        activation=activation,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "    )\n",
    "    model.add(dense)\n",
    "    opt = tf.keras.optimizers.Adam(lr=0.01)\n",
    "    model.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def get_keras_early_stopping(patience=10, monitor=\"val_accuracy\"):\n",
    "    \"\"\"Stops training if monitor value doesn't exceed the current max value after patience num of epochs\"\"\"\n",
    "    return tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=monitor, patience=patience, verbose=1, restore_best_weights=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not convert abstained vote to probability",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-df0450238a3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobs_train_filtered\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_to_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_keras_early_stopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/snorkel-notebooks/venv/lib/python3.7/site-packages/snorkel/utils/core.py\u001b[0m in \u001b[0;36mpreds_to_probs\u001b[0;34m(preds, num_classes)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \"\"\"\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not convert abstained vote to probability\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Could not convert abstained vote to probability"
     ]
    }
   ],
   "source": [
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from snorkel.analysis import metric_score\n",
    "    from snorkel.labeling import filter_unlabeled_dataframe\n",
    "    from snorkel.utils import preds_to_probs\n",
    "    import tensorflow as tf\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "    X_train = vectorizer.fit_transform(df_train_filtered.text.tolist())\n",
    "\n",
    "    X_valid = vectorizer.transform(df_valid[\"text\"].tolist())\n",
    "    X_test = vectorizer.transform(df_test[\"text\"].tolist())\n",
    "\n",
    "    Y_valid = (df_valid[\"label\"] - 1).values\n",
    "    Y_test = (df_test[\"label\"]-1).values\n",
    "\n",
    "    # Define a vanilla logistic regression model with Keras\n",
    "    keras_model = get_keras_logreg(input_dim=X_train.shape[1])\n",
    "\n",
    "    keras_model.fit(\n",
    "        x=X_train,\n",
    "        y=probs_train_filtered,\n",
    "        validation_data=(X_valid, preds_to_probs(Y_valid, 2)),\n",
    "        callbacks=[get_keras_early_stopping()],\n",
    "        epochs=50,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    preds_test = keras_model.predict(x=X_test).argmax(axis=1)\n",
    "    test_acc = metric_score(golds=Y_test, preds=preds_test, metric=\"accuracy\")\n",
    "    print(f\"Test Accuracy: {test_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.193"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_score(golds=Y_test, preds=preds_test, metric=\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save\n",
    "When your time is up, please save your explanations and model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 10 explanations to babbler_amazon_explanations.tsv\n",
      "Read 10 explanations from babbler_amazon_explanations.tsv\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "stat_history.to_csv(\"babbler_amazon_statistics_history.csv\")\n",
    "FILE = \"babbler_amazon_explanations.tsv\"\n",
    "exp_io = ExplanationIO()\n",
    "for exp in explanations:\n",
    "    if exp.candidate is None:\n",
    "        exp.candidate = SimpleNamespace(mention_id = None)\n",
    "exp_io.write(explanations, FILE)\n",
    "explanations = exp_io.read(FILE)\n",
    "label_aggregator.save(\"babble_amazon_lfmodel.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
